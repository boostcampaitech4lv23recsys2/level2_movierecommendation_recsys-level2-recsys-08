{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c53ab74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:51:07.340566Z",
     "start_time": "2022-12-23T06:51:01.208792Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "from tqdm import tqdm\n",
    "from logging import getLogger\n",
    "import torch\n",
    "\n",
    "from recbole.model.general_recommender.ease import EASE\n",
    "from recbole.model.context_aware_recommender.ffm import FFM\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation, Interaction\n",
    "from recbole.utils import init_logger, get_trainer, get_model, init_seed, set_color\n",
    "\n",
    "\n",
    "SEED=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c169d301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:51:07.603222Z",
     "start_time": "2022-12-23T06:51:07.343232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ml_item2attributes.json  genres.tsv  train_ratings.csv\tyears.tsv\n",
      "directors.tsv\t\t titles.tsv  writers.tsv\n",
      "/opt/ml/input/data/train/train_ratings.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/train\n",
    "!readlink -ef ../../data/train/train_ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba6987",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bbba30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:51:08.996003Z",
     "start_time": "2022-12-23T06:51:07.605399Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/opt/ml/input/data/train/train_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6954c698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:51:11.858012Z",
     "start_time": "2022-12-23T06:51:08.998778Z"
    }
   },
   "outputs": [],
   "source": [
    "user2idx = {v:k for k,v in enumerate(sorted(set(train.user)))}\n",
    "item2idx = {v:k for k,v in enumerate(sorted(set(train.item)))}\n",
    "uidx2user = {k:v for k,v in enumerate(sorted(set(train.user)))}\n",
    "iidx2item = {k:v for k,v in enumerate(sorted(set(train.item)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcea99e",
   "metadata": {},
   "source": [
    "## make inter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930038a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:51:12.280952Z",
     "start_time": "2022-12-23T06:51:11.860073Z"
    }
   },
   "outputs": [],
   "source": [
    "train.user = train.user.map(user2idx)\n",
    "train.item = train.item.map(item2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aed392f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:51:12.286795Z",
     "start_time": "2022-12-23T06:51:12.283008Z"
    }
   },
   "outputs": [],
   "source": [
    "train.columns=['user_id:token','item_id:token','timestamp:float']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d5f8092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:51:12.305595Z",
     "start_time": "2022-12-23T06:51:12.288827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2505</td>\n",
       "      <td>1230782529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>1230782534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id:token  item_id:token  timestamp:float\n",
       "0              0           2505       1230782529\n",
       "1              0            109       1230782534"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286c0b14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:53:41.566235Z",
     "start_time": "2022-12-23T06:53:32.937279Z"
    }
   },
   "outputs": [],
   "source": [
    "outpath = f\"dataset/train_data\"\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "# sub_train=train.groupby(\"user\").sample(n=10, random_state=SEED)\n",
    "# sub_train.shape\n",
    "train.to_csv(os.path.join(outpath,\"train_data.inter\"),sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8b45f",
   "metadata": {},
   "source": [
    "## make yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc936e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:53:41.578979Z",
     "start_time": "2022-12-23T06:53:41.574905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yamldata=\"\"\"\n",
    "USER_ID_FIELD: user_id\n",
    "ITEM_ID_FIELD: item_id\n",
    "TIME_FIELD: timestamp\n",
    "\n",
    "load_col:\n",
    "    inter: [user_id, item_id, timestamp]\n",
    "\"\"\"\n",
    "with open(\"ease.yaml\", \"w\") as f:\n",
    "    f.write(yamldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f6f4f",
   "metadata": {},
   "source": [
    "## make config, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41efa260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:53:42.771394Z",
     "start_time": "2022-12-23T06:53:41.581254Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26 Dec 11:47    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = dataset/train_data\n",
      "checkpoint_dir = saved\n",
      "show_progress = False\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 500\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [1, 0, 0]}, 'group_by': 'user', 'order': 'RO', 'mode': 'full'}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [20]\n",
      "valid_metric = Recall@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id', 'timestamp']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "mlp_hidden_size = [600, 400, 200]\n",
      "latent_dimension = 128\n",
      "dropout_prob = 0.5\n",
      "anneal_cap = 0.2\n",
      "total_anneal_steps = 200000\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cuda\n",
      "eval_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = getLogger()\n",
    "\n",
    "# configurations initialization\n",
    "config = Config(model='MultiVAE', dataset=\"train_data\", config_file_list=[f'ease.yaml'])\n",
    "config['epochs'] = 500\n",
    "config['show_progress'] = False\n",
    "config['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config['valid_metric'] = \"Recall@10\"\n",
    "config['eval_args'] = {'split': {'RS': [1, 0, 0]},\n",
    "                         'group_by': 'user',\n",
    "                         'order': 'RO',\n",
    "                         'mode': 'full'}\n",
    "config['topk']=[20]\n",
    "config['mlp_hidden_size'] = [600,400,200]\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "# logger initialization\n",
    "init_logger(config)\n",
    "logger.info(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4ee8a",
   "metadata": {},
   "source": [
    "## make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b8abda8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:54:25.066199Z",
     "start_time": "2022-12-23T06:53:43.271187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26 Dec 11:47    INFO  train_data\n",
      "The number of users: 31361\n",
      "Average actions of users: 164.36450892857144\n",
      "The number of items: 6808\n",
      "Average actions of items: 757.2309387395328\n",
      "The number of inters: 5154471\n",
      "The sparsity of the dataset: 97.58579218741939%\n",
      "Remain Fields: ['user_id', 'item_id', 'timestamp']\n",
      "26 Dec 11:48    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "26 Dec 11:48    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [1, 0, 0]}, 'group_by': 'user', 'order': 'RO', 'mode': 'full'}]\n"
     ]
    }
   ],
   "source": [
    "# dataset filtering\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)\n",
    "\n",
    "# dataset splitting\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90215db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:54:26.812308Z",
     "start_time": "2022-12-23T06:54:25.068691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtrain_data\u001b[0m\n",
       "\u001b[1;34mThe number of users\u001b[0m: 31361\n",
       "\u001b[1;34mAverage actions of users\u001b[0m: 164.36450892857144\n",
       "\u001b[1;34mThe number of items\u001b[0m: 6808\n",
       "\u001b[1;34mAverage actions of items\u001b[0m: 757.2309387395328\n",
       "\u001b[1;34mThe number of inters\u001b[0m: 5154471\n",
       "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 97.58579218741939%\n",
       "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtrain_data\u001b[0m\n",
       "\u001b[1;34mThe number of users\u001b[0m: 31361\n",
       "\u001b[1;34mAverage actions of users\u001b[0m: nan\n",
       "\u001b[1;34mThe number of items\u001b[0m: 6808\n",
       "\u001b[1;34mAverage actions of items\u001b[0m: nan\n",
       "\u001b[1;34mThe number of inters\u001b[0m: 0\n",
       "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 100.0%\n",
       "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtrain_data\u001b[0m\n",
       "\u001b[1;34mThe number of users\u001b[0m: 31361\n",
       "\u001b[1;34mAverage actions of users\u001b[0m: nan\n",
       "\u001b[1;34mThe number of items\u001b[0m: 6808\n",
       "\u001b[1;34mAverage actions of items\u001b[0m: nan\n",
       "\u001b[1;34mThe number of inters\u001b[0m: 0\n",
       "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 100.0%\n",
       "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dataset\n",
    "valid_data.dataset\n",
    "test_data.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19f483",
   "metadata": {},
   "source": [
    "## make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a444e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.model.general_recommender.multivae import MultiVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ee187fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:54:55.239195Z",
     "start_time": "2022-12-23T06:54:26.814385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26 Dec 11:50    WARNING  Max value of user's history interaction records has reached 42.773207990599296% of the total.\n",
      "26 Dec 11:50    INFO  MultiVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=6808, out_features=600, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=600, out_features=400, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=200, out_features=128, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=200, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=200, out_features=400, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=400, out_features=600, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=600, out_features=6808, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 8857336\n"
     ]
    }
   ],
   "source": [
    "# model loading and initialization\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "model = MultiVAE(config, train_data.dataset).to(config['device'])\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd695a",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a2e35d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:55:09.380447Z",
     "start_time": "2022-12-23T06:54:55.242022Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26 Dec 11:50    INFO  epoch 0 training [time: 0.21s, train loss: 20328.1908]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 1 training [time: 0.19s, train loss: 19994.5710]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 2 training [time: 0.19s, train loss: 19731.4634]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 3 training [time: 0.19s, train loss: 19577.1595]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 4 training [time: 0.19s, train loss: 19461.9850]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 5 training [time: 0.19s, train loss: 19346.0281]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 6 training [time: 0.19s, train loss: 19317.0836]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 7 training [time: 0.19s, train loss: 19193.9642]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 8 training [time: 0.19s, train loss: 19142.8065]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 9 training [time: 0.19s, train loss: 19088.2817]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 10 training [time: 0.19s, train loss: 18964.6738]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 11 training [time: 0.19s, train loss: 18934.6829]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 12 training [time: 0.19s, train loss: 18864.7268]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 13 training [time: 0.19s, train loss: 18836.5034]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 14 training [time: 0.19s, train loss: 18811.6221]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 15 training [time: 0.19s, train loss: 18738.0043]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:50    INFO  epoch 16 training [time: 0.19s, train loss: 18720.2167]\n",
      "26 Dec 11:50    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 17 training [time: 0.19s, train loss: 18651.9153]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 18 training [time: 0.19s, train loss: 18666.2457]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 19 training [time: 0.19s, train loss: 18617.9391]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 20 training [time: 0.19s, train loss: 18557.1637]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 21 training [time: 0.19s, train loss: 18611.7688]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 22 training [time: 0.19s, train loss: 18588.1265]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 23 training [time: 0.19s, train loss: 18474.5204]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 24 training [time: 0.19s, train loss: 18512.5304]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 25 training [time: 0.19s, train loss: 18499.0520]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 26 training [time: 0.19s, train loss: 18509.5996]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 27 training [time: 0.19s, train loss: 18483.7300]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 28 training [time: 0.19s, train loss: 18533.0546]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 29 training [time: 0.19s, train loss: 18460.9506]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 30 training [time: 0.19s, train loss: 18467.1255]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 31 training [time: 0.19s, train loss: 18396.9861]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 32 training [time: 0.19s, train loss: 18382.8470]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 33 training [time: 0.19s, train loss: 18361.8334]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 34 training [time: 0.19s, train loss: 18380.2020]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 35 training [time: 0.19s, train loss: 18414.4995]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 36 training [time: 0.19s, train loss: 18360.0953]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 37 training [time: 0.19s, train loss: 18362.3613]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 38 training [time: 0.19s, train loss: 18374.3265]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 39 training [time: 0.19s, train loss: 18336.5459]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 40 training [time: 0.19s, train loss: 18314.9480]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 41 training [time: 0.19s, train loss: 18323.8154]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 42 training [time: 0.19s, train loss: 18249.9408]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 43 training [time: 0.19s, train loss: 18216.8466]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 44 training [time: 0.19s, train loss: 18265.8044]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 45 training [time: 0.19s, train loss: 18262.8848]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 46 training [time: 0.19s, train loss: 18195.4772]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 47 training [time: 0.19s, train loss: 18206.7482]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 48 training [time: 0.19s, train loss: 18258.5436]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 49 training [time: 0.19s, train loss: 18232.7587]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 50 training [time: 0.19s, train loss: 18231.9105]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 51 training [time: 0.19s, train loss: 18165.1849]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 52 training [time: 0.19s, train loss: 18178.6631]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 53 training [time: 0.19s, train loss: 18178.7861]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 54 training [time: 0.19s, train loss: 18114.9141]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 55 training [time: 0.19s, train loss: 18119.0126]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 56 training [time: 0.19s, train loss: 18091.7250]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 57 training [time: 0.19s, train loss: 18096.3462]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 58 training [time: 0.19s, train loss: 18072.7662]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 59 training [time: 0.19s, train loss: 18138.8093]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 60 training [time: 0.19s, train loss: 18099.6295]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 61 training [time: 0.19s, train loss: 18051.0625]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 62 training [time: 0.19s, train loss: 18065.4607]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 63 training [time: 0.19s, train loss: 18085.4608]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 64 training [time: 0.19s, train loss: 18090.0088]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 65 training [time: 0.19s, train loss: 18030.2339]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 66 training [time: 0.19s, train loss: 18062.6050]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 67 training [time: 0.19s, train loss: 18045.9260]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 68 training [time: 0.19s, train loss: 18075.8258]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 69 training [time: 0.19s, train loss: 18059.3318]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 70 training [time: 0.19s, train loss: 18081.2878]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 71 training [time: 0.19s, train loss: 18093.8177]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 72 training [time: 0.19s, train loss: 18035.8046]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 73 training [time: 0.19s, train loss: 18035.0719]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 74 training [time: 0.19s, train loss: 17991.5088]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 75 training [time: 0.19s, train loss: 17995.3308]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 76 training [time: 0.19s, train loss: 17972.7423]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 77 training [time: 0.19s, train loss: 17997.5935]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 78 training [time: 0.19s, train loss: 18036.2190]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 79 training [time: 0.19s, train loss: 18004.6945]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 80 training [time: 0.19s, train loss: 17933.4019]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 81 training [time: 0.19s, train loss: 17953.9871]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 82 training [time: 0.19s, train loss: 17937.3300]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 83 training [time: 0.19s, train loss: 17953.3618]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 84 training [time: 0.19s, train loss: 17949.1396]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 85 training [time: 0.19s, train loss: 17893.8574]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 86 training [time: 0.19s, train loss: 17940.9822]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 87 training [time: 0.19s, train loss: 17915.2531]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 88 training [time: 0.19s, train loss: 17883.2249]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 89 training [time: 0.19s, train loss: 17917.2386]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 90 training [time: 0.19s, train loss: 17933.6299]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 91 training [time: 0.19s, train loss: 17953.9576]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 92 training [time: 0.19s, train loss: 17858.0999]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 93 training [time: 0.19s, train loss: 17912.4299]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 94 training [time: 0.19s, train loss: 17923.5764]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 95 training [time: 0.19s, train loss: 17908.7433]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 96 training [time: 0.19s, train loss: 17887.3523]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 97 training [time: 0.19s, train loss: 17880.9756]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 98 training [time: 0.19s, train loss: 17870.5337]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 99 training [time: 0.19s, train loss: 17865.9600]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 100 training [time: 0.19s, train loss: 17858.9969]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 101 training [time: 0.19s, train loss: 17810.5767]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 102 training [time: 0.19s, train loss: 17855.1021]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 103 training [time: 0.19s, train loss: 17859.7542]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 104 training [time: 0.19s, train loss: 17855.9210]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 105 training [time: 0.19s, train loss: 17843.8104]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 106 training [time: 0.19s, train loss: 17834.5963]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 107 training [time: 0.19s, train loss: 17844.4852]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 108 training [time: 0.19s, train loss: 17837.4640]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 109 training [time: 0.19s, train loss: 17864.2306]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 110 training [time: 0.19s, train loss: 17830.3406]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 111 training [time: 0.19s, train loss: 17784.4315]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 112 training [time: 0.19s, train loss: 17773.2117]\n",
      "26 Dec 11:51    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:51    INFO  epoch 113 training [time: 0.19s, train loss: 17740.5925]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 114 training [time: 0.19s, train loss: 17801.5875]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 115 training [time: 0.19s, train loss: 17747.5618]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 116 training [time: 0.19s, train loss: 17789.3065]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 117 training [time: 0.19s, train loss: 17781.0118]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 118 training [time: 0.19s, train loss: 17757.7551]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 119 training [time: 0.19s, train loss: 17703.7374]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 120 training [time: 0.19s, train loss: 17823.6591]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 121 training [time: 0.19s, train loss: 17776.3838]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 122 training [time: 0.19s, train loss: 17796.9257]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 123 training [time: 0.19s, train loss: 17752.3279]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 124 training [time: 0.19s, train loss: 17760.7699]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 125 training [time: 0.19s, train loss: 17737.0789]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 126 training [time: 0.19s, train loss: 17762.4327]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 127 training [time: 0.19s, train loss: 17783.1108]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 128 training [time: 0.19s, train loss: 17728.7145]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 129 training [time: 0.19s, train loss: 17704.1034]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 130 training [time: 0.19s, train loss: 17773.8851]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 131 training [time: 0.19s, train loss: 17747.8494]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 132 training [time: 0.19s, train loss: 17772.3269]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 133 training [time: 0.19s, train loss: 17745.8394]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 134 training [time: 0.19s, train loss: 17698.1744]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 135 training [time: 0.19s, train loss: 17686.7163]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 136 training [time: 0.19s, train loss: 17717.2196]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 137 training [time: 0.19s, train loss: 17762.0016]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 138 training [time: 0.19s, train loss: 17692.0106]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 139 training [time: 0.19s, train loss: 17684.0576]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 140 training [time: 0.19s, train loss: 17763.1212]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 141 training [time: 0.19s, train loss: 17648.2954]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 142 training [time: 0.19s, train loss: 17686.5090]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 143 training [time: 0.19s, train loss: 17687.2677]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 144 training [time: 0.19s, train loss: 17677.2988]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 145 training [time: 0.19s, train loss: 17664.0065]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 146 training [time: 0.19s, train loss: 17685.9585]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 147 training [time: 0.19s, train loss: 17660.5634]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 148 training [time: 0.19s, train loss: 17704.0183]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 149 training [time: 0.19s, train loss: 17727.3581]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 150 training [time: 0.19s, train loss: 17693.4229]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 151 training [time: 0.19s, train loss: 17742.1741]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 152 training [time: 0.19s, train loss: 17717.3213]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 153 training [time: 0.19s, train loss: 17637.2994]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 154 training [time: 0.19s, train loss: 17645.0623]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 155 training [time: 0.19s, train loss: 17638.4644]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 156 training [time: 0.19s, train loss: 17703.5006]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 157 training [time: 0.19s, train loss: 17612.5830]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 158 training [time: 0.19s, train loss: 17638.3459]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 159 training [time: 0.19s, train loss: 17675.8169]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 160 training [time: 0.19s, train loss: 17693.7666]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 161 training [time: 0.19s, train loss: 17719.5878]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 162 training [time: 0.19s, train loss: 17643.8160]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 163 training [time: 0.19s, train loss: 17596.6796]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 164 training [time: 0.19s, train loss: 17646.8898]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 165 training [time: 0.19s, train loss: 17709.8556]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 166 training [time: 0.19s, train loss: 17624.2003]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 167 training [time: 0.19s, train loss: 17622.0142]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 168 training [time: 0.19s, train loss: 17603.4064]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 169 training [time: 0.19s, train loss: 17564.5944]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 170 training [time: 0.19s, train loss: 17649.9246]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 171 training [time: 0.19s, train loss: 17586.7432]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 172 training [time: 0.19s, train loss: 17581.1316]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 173 training [time: 0.19s, train loss: 17594.8013]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 174 training [time: 0.19s, train loss: 17651.5601]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 175 training [time: 0.19s, train loss: 17620.9764]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 176 training [time: 0.19s, train loss: 17598.6630]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 177 training [time: 0.19s, train loss: 17621.6829]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 178 training [time: 0.19s, train loss: 17662.0802]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 179 training [time: 0.19s, train loss: 17690.1957]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 180 training [time: 0.19s, train loss: 17549.8396]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 181 training [time: 0.19s, train loss: 17595.5160]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 182 training [time: 0.19s, train loss: 17548.9816]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 183 training [time: 0.19s, train loss: 17631.5094]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 184 training [time: 0.19s, train loss: 17511.2559]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 185 training [time: 0.19s, train loss: 17578.7754]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 186 training [time: 0.19s, train loss: 17577.1837]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 187 training [time: 0.19s, train loss: 17619.6710]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 188 training [time: 0.19s, train loss: 17610.0316]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 189 training [time: 0.19s, train loss: 17538.5815]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 190 training [time: 0.19s, train loss: 17582.0013]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 191 training [time: 0.19s, train loss: 17568.1469]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 192 training [time: 0.19s, train loss: 17580.4146]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 193 training [time: 0.19s, train loss: 17561.7394]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 194 training [time: 0.19s, train loss: 17550.7899]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 195 training [time: 0.19s, train loss: 17534.9652]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 196 training [time: 0.19s, train loss: 17589.6638]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 197 training [time: 0.19s, train loss: 17596.6603]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 198 training [time: 0.19s, train loss: 17532.8188]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 199 training [time: 0.19s, train loss: 17575.1309]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 200 training [time: 0.19s, train loss: 17521.7955]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 201 training [time: 0.19s, train loss: 17558.9117]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 202 training [time: 0.19s, train loss: 17562.1672]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 203 training [time: 0.19s, train loss: 17517.2301]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 204 training [time: 0.19s, train loss: 17519.8568]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 205 training [time: 0.19s, train loss: 17519.2312]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 206 training [time: 0.19s, train loss: 17575.5846]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 207 training [time: 0.19s, train loss: 17506.2155]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 208 training [time: 0.19s, train loss: 17482.4590]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 209 training [time: 0.19s, train loss: 17515.9229]\n",
      "26 Dec 11:52    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:52    INFO  epoch 210 training [time: 0.19s, train loss: 17504.4759]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 211 training [time: 0.19s, train loss: 17568.2729]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 212 training [time: 0.19s, train loss: 17487.4490]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 213 training [time: 0.19s, train loss: 17507.9442]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 214 training [time: 0.19s, train loss: 17513.8397]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 215 training [time: 0.19s, train loss: 17538.5321]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 216 training [time: 0.19s, train loss: 17521.3302]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 217 training [time: 0.19s, train loss: 17547.9092]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 218 training [time: 0.19s, train loss: 17525.3048]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 219 training [time: 0.19s, train loss: 17481.4923]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 220 training [time: 0.19s, train loss: 17480.8722]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 221 training [time: 0.19s, train loss: 17506.7681]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 222 training [time: 0.19s, train loss: 17478.9696]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 223 training [time: 0.19s, train loss: 17497.4952]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 224 training [time: 0.19s, train loss: 17485.3401]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 225 training [time: 0.19s, train loss: 17482.4945]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 226 training [time: 0.19s, train loss: 17437.6293]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 227 training [time: 0.19s, train loss: 17498.6970]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 228 training [time: 0.19s, train loss: 17434.9306]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 229 training [time: 0.19s, train loss: 17536.5214]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 230 training [time: 0.19s, train loss: 17432.5805]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 231 training [time: 0.19s, train loss: 17544.4268]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 232 training [time: 0.19s, train loss: 17474.7588]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 233 training [time: 0.19s, train loss: 17452.1221]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 234 training [time: 0.19s, train loss: 17472.7449]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 235 training [time: 0.19s, train loss: 17488.2737]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 236 training [time: 0.19s, train loss: 17443.0812]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 237 training [time: 0.19s, train loss: 17498.2081]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 238 training [time: 0.19s, train loss: 17442.2439]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 239 training [time: 0.19s, train loss: 17503.4808]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 240 training [time: 0.19s, train loss: 17471.2969]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 241 training [time: 0.19s, train loss: 17503.7285]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 242 training [time: 0.19s, train loss: 17448.0399]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 243 training [time: 0.19s, train loss: 17469.1050]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 244 training [time: 0.19s, train loss: 17509.4392]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 245 training [time: 0.19s, train loss: 17477.9633]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 246 training [time: 0.19s, train loss: 17435.2512]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 247 training [time: 0.19s, train loss: 17415.3135]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 248 training [time: 0.19s, train loss: 17451.4189]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 249 training [time: 0.19s, train loss: 17448.2922]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 250 training [time: 0.19s, train loss: 17478.2725]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 251 training [time: 0.19s, train loss: 17479.7559]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 252 training [time: 0.19s, train loss: 17448.4729]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 253 training [time: 0.19s, train loss: 17454.1431]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 254 training [time: 0.19s, train loss: 17397.4525]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 255 training [time: 0.19s, train loss: 17381.9999]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 256 training [time: 0.19s, train loss: 17421.1047]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 257 training [time: 0.19s, train loss: 17427.4628]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 258 training [time: 0.19s, train loss: 17426.4170]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 259 training [time: 0.19s, train loss: 17422.3156]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 260 training [time: 0.19s, train loss: 17416.3038]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 261 training [time: 0.19s, train loss: 17464.2189]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 262 training [time: 0.19s, train loss: 17445.3021]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 263 training [time: 0.19s, train loss: 17420.3617]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 264 training [time: 0.19s, train loss: 17378.7845]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 265 training [time: 0.19s, train loss: 17434.3517]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 266 training [time: 0.19s, train loss: 17462.8926]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 267 training [time: 0.19s, train loss: 17441.0027]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 268 training [time: 0.19s, train loss: 17389.7826]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 269 training [time: 0.19s, train loss: 17410.6136]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 270 training [time: 0.19s, train loss: 17408.5848]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 271 training [time: 0.19s, train loss: 17387.5547]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 272 training [time: 0.19s, train loss: 17375.2339]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 273 training [time: 0.19s, train loss: 17416.4647]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 274 training [time: 0.19s, train loss: 17380.5021]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 275 training [time: 0.19s, train loss: 17473.4950]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 276 training [time: 0.19s, train loss: 17426.7770]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 277 training [time: 0.19s, train loss: 17423.8650]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 278 training [time: 0.19s, train loss: 17401.5133]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 279 training [time: 0.19s, train loss: 17383.0936]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 280 training [time: 0.19s, train loss: 17448.4540]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 281 training [time: 0.19s, train loss: 17375.7614]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 282 training [time: 0.19s, train loss: 17398.0947]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 283 training [time: 0.19s, train loss: 17392.5446]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 284 training [time: 0.19s, train loss: 17386.0321]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 285 training [time: 0.19s, train loss: 17419.2644]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 286 training [time: 0.19s, train loss: 17422.5051]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 287 training [time: 0.19s, train loss: 17381.3375]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 288 training [time: 0.19s, train loss: 17363.9203]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 289 training [time: 0.19s, train loss: 17350.2366]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 290 training [time: 0.19s, train loss: 17433.7924]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 291 training [time: 0.22s, train loss: 17382.5787]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 292 training [time: 0.19s, train loss: 17349.6816]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 293 training [time: 0.19s, train loss: 17392.1095]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 294 training [time: 0.19s, train loss: 17410.8352]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 295 training [time: 0.19s, train loss: 17415.2234]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 296 training [time: 0.19s, train loss: 17394.3699]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 297 training [time: 0.19s, train loss: 17372.1549]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 298 training [time: 0.19s, train loss: 17340.6520]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 299 training [time: 0.19s, train loss: 17386.3770]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 300 training [time: 0.19s, train loss: 17420.7136]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 301 training [time: 0.19s, train loss: 17401.1080]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 302 training [time: 0.19s, train loss: 17338.4344]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 303 training [time: 0.19s, train loss: 17330.6349]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 304 training [time: 0.19s, train loss: 17367.3254]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 305 training [time: 0.19s, train loss: 17322.5677]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:53    INFO  epoch 306 training [time: 0.19s, train loss: 17351.1642]\n",
      "26 Dec 11:53    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 307 training [time: 0.19s, train loss: 17387.1122]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 308 training [time: 0.19s, train loss: 17344.4763]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 309 training [time: 0.19s, train loss: 17374.3484]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 310 training [time: 0.19s, train loss: 17342.4706]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 311 training [time: 0.19s, train loss: 17294.4420]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 312 training [time: 0.19s, train loss: 17403.2432]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 313 training [time: 0.19s, train loss: 17381.0167]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 314 training [time: 0.19s, train loss: 17377.5851]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 315 training [time: 0.19s, train loss: 17359.3502]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 316 training [time: 0.19s, train loss: 17304.6793]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 317 training [time: 0.19s, train loss: 17363.9221]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 318 training [time: 0.19s, train loss: 17348.6631]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 319 training [time: 0.19s, train loss: 17353.2288]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 320 training [time: 0.19s, train loss: 17390.2023]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 321 training [time: 0.19s, train loss: 17404.4673]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 322 training [time: 0.19s, train loss: 17360.5304]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 323 training [time: 0.19s, train loss: 17348.0789]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 324 training [time: 0.19s, train loss: 17355.3409]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 325 training [time: 0.19s, train loss: 17325.3572]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 326 training [time: 0.19s, train loss: 17318.7023]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 327 training [time: 0.19s, train loss: 17348.7504]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 328 training [time: 0.19s, train loss: 17342.9026]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 329 training [time: 0.19s, train loss: 17340.4839]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 330 training [time: 0.19s, train loss: 17325.7848]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 331 training [time: 0.19s, train loss: 17322.0603]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 332 training [time: 0.19s, train loss: 17339.2567]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 333 training [time: 0.19s, train loss: 17319.8553]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 334 training [time: 0.19s, train loss: 17370.4237]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 335 training [time: 0.19s, train loss: 17330.5674]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 336 training [time: 0.19s, train loss: 17326.6254]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 337 training [time: 0.19s, train loss: 17325.6273]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 338 training [time: 0.19s, train loss: 17356.0192]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 339 training [time: 0.19s, train loss: 17359.3875]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 340 training [time: 0.19s, train loss: 17352.7018]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 341 training [time: 0.19s, train loss: 17290.8832]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 342 training [time: 0.19s, train loss: 17356.9100]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 343 training [time: 0.19s, train loss: 17292.6495]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 344 training [time: 0.19s, train loss: 17367.8546]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 345 training [time: 0.19s, train loss: 17337.7164]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 346 training [time: 0.19s, train loss: 17287.4988]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 347 training [time: 0.19s, train loss: 17344.3076]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 348 training [time: 0.19s, train loss: 17334.2371]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 349 training [time: 0.19s, train loss: 17337.3272]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 350 training [time: 0.19s, train loss: 17333.0197]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 351 training [time: 0.19s, train loss: 17321.1514]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 352 training [time: 0.19s, train loss: 17327.1653]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 353 training [time: 0.19s, train loss: 17276.0195]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 354 training [time: 0.19s, train loss: 17303.0369]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 355 training [time: 0.19s, train loss: 17388.1185]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 356 training [time: 0.19s, train loss: 17346.0549]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 357 training [time: 0.19s, train loss: 17307.7133]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 358 training [time: 0.19s, train loss: 17327.5449]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 359 training [time: 0.19s, train loss: 17282.0151]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 360 training [time: 0.19s, train loss: 17311.9685]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 361 training [time: 0.19s, train loss: 17338.3151]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 362 training [time: 0.19s, train loss: 17341.3104]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 363 training [time: 0.19s, train loss: 17319.9385]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 364 training [time: 0.19s, train loss: 17273.2763]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 365 training [time: 0.19s, train loss: 17317.4061]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 366 training [time: 0.19s, train loss: 17357.2480]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 367 training [time: 0.19s, train loss: 17324.3672]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 368 training [time: 0.19s, train loss: 17307.3085]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 369 training [time: 0.19s, train loss: 17295.4076]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 370 training [time: 0.19s, train loss: 17321.6786]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 371 training [time: 0.19s, train loss: 17301.3845]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 372 training [time: 0.19s, train loss: 17386.0804]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 373 training [time: 0.19s, train loss: 17279.8677]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 374 training [time: 0.19s, train loss: 17251.0878]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 375 training [time: 0.19s, train loss: 17251.0785]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 376 training [time: 0.19s, train loss: 17298.0873]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 377 training [time: 0.19s, train loss: 17295.6991]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 378 training [time: 0.19s, train loss: 17338.8279]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 379 training [time: 0.19s, train loss: 17252.7346]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 380 training [time: 0.19s, train loss: 17337.6924]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 381 training [time: 0.19s, train loss: 17283.5378]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 382 training [time: 0.19s, train loss: 17270.8318]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 383 training [time: 0.20s, train loss: 17269.1917]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 384 training [time: 0.19s, train loss: 17342.6272]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 385 training [time: 0.19s, train loss: 17271.0997]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 386 training [time: 0.19s, train loss: 17343.3177]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 387 training [time: 0.19s, train loss: 17282.1693]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 388 training [time: 0.19s, train loss: 17282.3964]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 389 training [time: 0.19s, train loss: 17297.7947]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 390 training [time: 0.19s, train loss: 17311.0880]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 391 training [time: 0.19s, train loss: 17319.1069]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 392 training [time: 0.19s, train loss: 17237.9655]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 393 training [time: 0.19s, train loss: 17322.6583]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 394 training [time: 0.19s, train loss: 17275.0442]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 395 training [time: 0.19s, train loss: 17276.4406]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 396 training [time: 0.19s, train loss: 17301.4213]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 397 training [time: 0.19s, train loss: 17316.6251]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 398 training [time: 0.19s, train loss: 17272.3005]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 399 training [time: 0.19s, train loss: 17324.9194]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 400 training [time: 0.19s, train loss: 17261.2216]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 401 training [time: 0.19s, train loss: 17298.2162]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 402 training [time: 0.19s, train loss: 17293.4816]\n",
      "26 Dec 11:54    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:54    INFO  epoch 403 training [time: 0.19s, train loss: 17253.7245]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 404 training [time: 0.19s, train loss: 17261.0288]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 405 training [time: 0.19s, train loss: 17269.3627]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 406 training [time: 0.19s, train loss: 17232.3309]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 407 training [time: 0.19s, train loss: 17231.2424]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 408 training [time: 0.19s, train loss: 17248.5504]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 409 training [time: 0.19s, train loss: 17253.3475]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 410 training [time: 0.19s, train loss: 17339.5012]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 411 training [time: 0.19s, train loss: 17268.5685]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 412 training [time: 0.19s, train loss: 17296.5977]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 413 training [time: 0.19s, train loss: 17238.2999]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 414 training [time: 0.19s, train loss: 17228.2478]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 415 training [time: 0.19s, train loss: 17344.4817]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 416 training [time: 0.19s, train loss: 17239.1669]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 417 training [time: 0.19s, train loss: 17302.0261]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 418 training [time: 0.19s, train loss: 17274.1847]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 419 training [time: 0.19s, train loss: 17294.2318]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 420 training [time: 0.19s, train loss: 17279.6150]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 421 training [time: 0.19s, train loss: 17257.2257]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 422 training [time: 0.19s, train loss: 17276.3060]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 423 training [time: 0.19s, train loss: 17225.7460]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 424 training [time: 0.19s, train loss: 17282.3234]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 425 training [time: 0.19s, train loss: 17254.8922]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 426 training [time: 0.19s, train loss: 17262.3057]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 427 training [time: 0.19s, train loss: 17294.4739]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 428 training [time: 0.19s, train loss: 17236.2532]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 429 training [time: 0.19s, train loss: 17279.8593]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 430 training [time: 0.19s, train loss: 17320.0709]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 431 training [time: 0.19s, train loss: 17287.6365]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 432 training [time: 0.19s, train loss: 17286.3964]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 433 training [time: 0.19s, train loss: 17242.1647]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 434 training [time: 0.19s, train loss: 17322.1423]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 435 training [time: 0.19s, train loss: 17274.2155]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 436 training [time: 0.19s, train loss: 17233.4015]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 437 training [time: 0.19s, train loss: 17244.5696]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 438 training [time: 0.19s, train loss: 17240.2419]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 439 training [time: 0.19s, train loss: 17268.2709]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 440 training [time: 0.19s, train loss: 17289.4003]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 441 training [time: 0.19s, train loss: 17301.0140]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 442 training [time: 0.19s, train loss: 17226.3574]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 443 training [time: 0.19s, train loss: 17244.1393]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 444 training [time: 0.19s, train loss: 17257.4891]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 445 training [time: 0.19s, train loss: 17245.2681]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 446 training [time: 0.19s, train loss: 17276.9373]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 447 training [time: 0.19s, train loss: 17242.6793]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 448 training [time: 0.19s, train loss: 17256.2991]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 449 training [time: 0.19s, train loss: 17256.9388]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 450 training [time: 0.19s, train loss: 17209.9714]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 451 training [time: 0.19s, train loss: 17258.3873]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 452 training [time: 0.19s, train loss: 17250.3557]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 453 training [time: 0.19s, train loss: 17218.2994]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 454 training [time: 0.19s, train loss: 17290.5654]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 455 training [time: 0.19s, train loss: 17310.3696]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 456 training [time: 0.19s, train loss: 17245.4659]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 457 training [time: 0.19s, train loss: 17239.8409]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 458 training [time: 0.19s, train loss: 17292.0580]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 459 training [time: 0.19s, train loss: 17257.5642]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 460 training [time: 0.19s, train loss: 17297.4363]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 461 training [time: 0.19s, train loss: 17227.3615]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 462 training [time: 0.19s, train loss: 17277.0756]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 463 training [time: 0.19s, train loss: 17253.8513]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 464 training [time: 0.19s, train loss: 17217.7944]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 465 training [time: 0.19s, train loss: 17246.1855]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 466 training [time: 0.19s, train loss: 17248.7843]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 467 training [time: 0.19s, train loss: 17219.8341]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 468 training [time: 0.19s, train loss: 17270.9659]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 469 training [time: 0.19s, train loss: 17245.7312]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 470 training [time: 0.19s, train loss: 17221.3500]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 471 training [time: 0.19s, train loss: 17269.5133]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 472 training [time: 0.19s, train loss: 17219.6693]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 473 training [time: 0.19s, train loss: 17201.2904]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 474 training [time: 0.19s, train loss: 17261.4336]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 475 training [time: 0.19s, train loss: 17234.5245]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 476 training [time: 0.19s, train loss: 17261.6703]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 477 training [time: 0.19s, train loss: 17289.0439]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 478 training [time: 0.19s, train loss: 17245.8353]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 479 training [time: 0.19s, train loss: 17240.1606]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 480 training [time: 0.19s, train loss: 17256.1892]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 481 training [time: 0.19s, train loss: 17273.9265]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 482 training [time: 0.19s, train loss: 17270.8132]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 483 training [time: 0.19s, train loss: 17192.9440]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 484 training [time: 0.19s, train loss: 17177.1155]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 485 training [time: 0.19s, train loss: 17189.5723]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 486 training [time: 0.19s, train loss: 17189.3907]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 487 training [time: 0.19s, train loss: 17203.4347]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 488 training [time: 0.19s, train loss: 17197.6764]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 489 training [time: 0.19s, train loss: 17254.8853]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 490 training [time: 0.19s, train loss: 17265.4907]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 491 training [time: 0.19s, train loss: 17225.5238]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 492 training [time: 0.19s, train loss: 17230.1173]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 493 training [time: 0.19s, train loss: 17188.6130]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 494 training [time: 0.19s, train loss: 17179.4115]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 495 training [time: 0.19s, train loss: 17252.2301]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 496 training [time: 0.19s, train loss: 17209.3096]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 497 training [time: 0.19s, train loss: 17231.8950]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 498 training [time: 0.19s, train loss: 17242.4308]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n",
      "26 Dec 11:55    INFO  epoch 499 training [time: 0.19s, train loss: 17187.3936]\n",
      "26 Dec 11:55    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-50-49.pth\n"
     ]
    }
   ],
   "source": [
    "# trainer loading and initialization\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data, valid_data, saved=True, show_progress=config['show_progress']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93b8e13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:29:03.667702Z",
     "start_time": "2022-12-21T16:28:47.580417Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26 Dec 11:56    INFO  epoch 0 training [time: 0.21s, train loss: 17476.6086]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 1 training [time: 0.20s, train loss: 17215.7827]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 2 training [time: 0.19s, train loss: 17284.5978]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 3 training [time: 0.19s, train loss: 17186.6301]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 4 training [time: 0.19s, train loss: 17266.6606]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 5 training [time: 0.19s, train loss: 17230.7141]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 6 training [time: 0.19s, train loss: 17273.0332]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 7 training [time: 0.19s, train loss: 17256.4016]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 8 training [time: 0.19s, train loss: 17193.8302]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 9 training [time: 0.19s, train loss: 17191.3226]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 10 training [time: 0.19s, train loss: 17240.9923]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 11 training [time: 0.19s, train loss: 17230.2609]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 12 training [time: 0.19s, train loss: 17217.1046]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 13 training [time: 0.19s, train loss: 17240.7930]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 14 training [time: 0.19s, train loss: 17213.4554]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 15 training [time: 0.19s, train loss: 17215.0084]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 16 training [time: 0.19s, train loss: 17230.6678]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 17 training [time: 0.19s, train loss: 17188.2233]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 18 training [time: 0.19s, train loss: 17214.2039]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 19 training [time: 0.19s, train loss: 17256.6807]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 20 training [time: 0.19s, train loss: 17203.5924]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 21 training [time: 0.19s, train loss: 17212.2078]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 22 training [time: 0.19s, train loss: 17179.1086]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 23 training [time: 0.19s, train loss: 17238.1882]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 24 training [time: 0.19s, train loss: 17217.8604]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 25 training [time: 0.19s, train loss: 17195.4161]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 26 training [time: 0.19s, train loss: 17211.1605]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 27 training [time: 0.19s, train loss: 17188.4800]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 28 training [time: 0.19s, train loss: 17220.6283]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 29 training [time: 0.19s, train loss: 17198.2405]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 30 training [time: 0.19s, train loss: 17201.1075]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 31 training [time: 0.19s, train loss: 17270.0652]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 32 training [time: 0.19s, train loss: 17207.9381]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 33 training [time: 0.19s, train loss: 17190.9728]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 34 training [time: 0.19s, train loss: 17255.1367]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 35 training [time: 0.19s, train loss: 17190.1454]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 36 training [time: 0.19s, train loss: 17178.3763]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 37 training [time: 0.19s, train loss: 17176.8519]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 38 training [time: 0.19s, train loss: 17235.8823]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 39 training [time: 0.19s, train loss: 17201.3586]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 40 training [time: 0.19s, train loss: 17192.1039]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 41 training [time: 0.19s, train loss: 17196.4251]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 42 training [time: 0.19s, train loss: 17218.1481]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 43 training [time: 0.19s, train loss: 17265.5653]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 44 training [time: 0.19s, train loss: 17200.1561]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 45 training [time: 0.19s, train loss: 17232.8237]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 46 training [time: 0.19s, train loss: 17217.3241]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 47 training [time: 0.19s, train loss: 17248.2599]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 48 training [time: 0.19s, train loss: 17230.0358]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 49 training [time: 0.19s, train loss: 17190.6647]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 50 training [time: 0.19s, train loss: 17177.0964]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 51 training [time: 0.19s, train loss: 17166.8329]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 52 training [time: 0.19s, train loss: 17213.1310]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 53 training [time: 0.19s, train loss: 17205.4127]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 54 training [time: 0.19s, train loss: 17195.6791]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 55 training [time: 0.19s, train loss: 17201.7034]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 56 training [time: 0.19s, train loss: 17219.8566]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 57 training [time: 0.19s, train loss: 17267.8326]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 58 training [time: 0.19s, train loss: 17254.6971]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 59 training [time: 0.19s, train loss: 17187.0934]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 60 training [time: 0.19s, train loss: 17221.5439]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 61 training [time: 0.19s, train loss: 17180.9630]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 62 training [time: 0.19s, train loss: 17211.0094]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 63 training [time: 0.19s, train loss: 17228.6001]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 64 training [time: 0.19s, train loss: 17222.8544]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 65 training [time: 0.19s, train loss: 17169.3256]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 66 training [time: 0.19s, train loss: 17290.4111]\n",
      "26 Dec 11:56    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:56    INFO  epoch 67 training [time: 0.19s, train loss: 17187.6211]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 68 training [time: 0.19s, train loss: 17175.4543]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 69 training [time: 0.19s, train loss: 17195.3921]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 70 training [time: 0.19s, train loss: 17169.1737]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 71 training [time: 0.19s, train loss: 17249.1738]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 72 training [time: 0.19s, train loss: 17193.6071]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 73 training [time: 0.19s, train loss: 17176.0486]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 74 training [time: 0.19s, train loss: 17157.9974]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 75 training [time: 0.19s, train loss: 17216.5735]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 76 training [time: 0.19s, train loss: 17202.9777]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 77 training [time: 0.19s, train loss: 17188.7749]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 78 training [time: 0.19s, train loss: 17159.4045]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 79 training [time: 0.19s, train loss: 17185.0134]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 80 training [time: 0.19s, train loss: 17211.2280]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 81 training [time: 0.19s, train loss: 17220.0868]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 82 training [time: 0.19s, train loss: 17187.6150]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 83 training [time: 0.19s, train loss: 17164.0830]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 84 training [time: 0.19s, train loss: 17186.4463]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 85 training [time: 0.19s, train loss: 17226.1036]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 86 training [time: 0.19s, train loss: 17189.4218]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 87 training [time: 0.19s, train loss: 17193.8535]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 88 training [time: 0.19s, train loss: 17131.4213]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 89 training [time: 0.19s, train loss: 17264.4214]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 90 training [time: 0.19s, train loss: 17193.1134]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 91 training [time: 0.19s, train loss: 17183.5496]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 92 training [time: 0.19s, train loss: 17232.3311]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 93 training [time: 0.19s, train loss: 17226.0889]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 94 training [time: 0.19s, train loss: 17130.9016]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 95 training [time: 0.19s, train loss: 17192.9318]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 96 training [time: 0.19s, train loss: 17223.7398]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 97 training [time: 0.19s, train loss: 17152.7375]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 98 training [time: 0.19s, train loss: 17204.6380]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 99 training [time: 0.19s, train loss: 17165.4106]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 100 training [time: 0.19s, train loss: 17212.0847]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 101 training [time: 0.19s, train loss: 17175.7269]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 102 training [time: 0.19s, train loss: 17130.4702]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 103 training [time: 0.19s, train loss: 17193.7781]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 104 training [time: 0.19s, train loss: 17201.0618]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 105 training [time: 0.19s, train loss: 17136.4465]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 106 training [time: 0.19s, train loss: 17189.5626]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 107 training [time: 0.19s, train loss: 17160.8335]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 108 training [time: 0.19s, train loss: 17194.7791]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 109 training [time: 0.19s, train loss: 17193.1189]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 110 training [time: 0.19s, train loss: 17144.8687]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 111 training [time: 0.19s, train loss: 17150.3031]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 112 training [time: 0.19s, train loss: 17206.3729]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 113 training [time: 0.19s, train loss: 17207.8682]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 114 training [time: 0.19s, train loss: 17171.1990]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 115 training [time: 0.19s, train loss: 17134.4349]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 116 training [time: 0.19s, train loss: 17125.4030]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 117 training [time: 0.19s, train loss: 17204.5754]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 118 training [time: 0.19s, train loss: 17197.3330]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 119 training [time: 0.19s, train loss: 17187.6760]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 120 training [time: 0.19s, train loss: 17149.2320]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 121 training [time: 0.19s, train loss: 17180.2892]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 122 training [time: 0.19s, train loss: 17180.2775]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 123 training [time: 0.19s, train loss: 17183.4736]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 124 training [time: 0.19s, train loss: 17160.0837]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 125 training [time: 0.19s, train loss: 17126.2581]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 126 training [time: 0.19s, train loss: 17140.1266]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 127 training [time: 0.19s, train loss: 17214.0864]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 128 training [time: 0.19s, train loss: 17154.4916]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 129 training [time: 0.19s, train loss: 17153.5551]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 130 training [time: 0.19s, train loss: 17205.8568]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 131 training [time: 0.19s, train loss: 17188.8339]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 132 training [time: 0.19s, train loss: 17231.7271]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 133 training [time: 0.19s, train loss: 17212.3212]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 134 training [time: 0.19s, train loss: 17232.3442]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 135 training [time: 0.19s, train loss: 17168.0953]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 136 training [time: 0.19s, train loss: 17204.3630]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 137 training [time: 0.19s, train loss: 17150.3102]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 138 training [time: 0.19s, train loss: 17200.8450]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 139 training [time: 0.19s, train loss: 17184.9672]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 140 training [time: 0.19s, train loss: 17168.9500]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 141 training [time: 0.19s, train loss: 17137.9137]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 142 training [time: 0.19s, train loss: 17167.9586]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 143 training [time: 0.19s, train loss: 17192.8885]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 144 training [time: 0.19s, train loss: 17179.9299]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 145 training [time: 0.19s, train loss: 17176.4974]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 146 training [time: 0.19s, train loss: 17136.6935]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 147 training [time: 0.19s, train loss: 17156.2271]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 148 training [time: 0.19s, train loss: 17149.0374]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 149 training [time: 0.19s, train loss: 17174.0347]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 150 training [time: 0.19s, train loss: 17174.7417]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 151 training [time: 0.19s, train loss: 17151.4417]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 152 training [time: 0.19s, train loss: 17162.2568]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 153 training [time: 0.19s, train loss: 17221.3549]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 154 training [time: 0.19s, train loss: 17144.3075]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 155 training [time: 0.19s, train loss: 17161.8751]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 156 training [time: 0.19s, train loss: 17212.9409]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 157 training [time: 0.19s, train loss: 17139.9281]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 158 training [time: 0.19s, train loss: 17142.9280]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 159 training [time: 0.19s, train loss: 17142.7563]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 160 training [time: 0.19s, train loss: 17127.8753]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 161 training [time: 0.19s, train loss: 17151.6005]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 162 training [time: 0.19s, train loss: 17156.5988]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 163 training [time: 0.19s, train loss: 17163.7854]\n",
      "26 Dec 11:57    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:57    INFO  epoch 164 training [time: 0.19s, train loss: 17150.6162]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 165 training [time: 0.19s, train loss: 17142.8920]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 166 training [time: 0.19s, train loss: 17129.8344]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 167 training [time: 0.19s, train loss: 17186.1263]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 168 training [time: 0.21s, train loss: 17139.1694]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 169 training [time: 0.19s, train loss: 17167.4847]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 170 training [time: 0.19s, train loss: 17160.7330]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 171 training [time: 0.19s, train loss: 17173.8394]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 172 training [time: 0.19s, train loss: 17205.6850]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 173 training [time: 0.19s, train loss: 17127.6749]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 174 training [time: 0.19s, train loss: 17200.0973]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 175 training [time: 0.19s, train loss: 17159.5554]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 176 training [time: 0.19s, train loss: 17142.5507]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 177 training [time: 0.19s, train loss: 17219.3392]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 178 training [time: 0.19s, train loss: 17118.1257]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 179 training [time: 0.19s, train loss: 17183.7601]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 180 training [time: 0.19s, train loss: 17137.6451]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 181 training [time: 0.19s, train loss: 17180.6201]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 182 training [time: 0.19s, train loss: 17118.8053]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 183 training [time: 0.19s, train loss: 17148.8311]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 184 training [time: 0.19s, train loss: 17141.1759]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 185 training [time: 0.19s, train loss: 17194.0785]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 186 training [time: 0.19s, train loss: 17141.1384]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 187 training [time: 0.19s, train loss: 17151.2977]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 188 training [time: 0.19s, train loss: 17181.5251]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 189 training [time: 0.19s, train loss: 17145.5499]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 190 training [time: 0.19s, train loss: 17184.8456]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 191 training [time: 0.19s, train loss: 17148.0254]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 192 training [time: 0.19s, train loss: 17194.6127]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 193 training [time: 0.19s, train loss: 17172.5515]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 194 training [time: 0.19s, train loss: 17077.1583]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 195 training [time: 0.19s, train loss: 17150.2064]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 196 training [time: 0.19s, train loss: 17208.3960]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 197 training [time: 0.19s, train loss: 17124.0559]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 198 training [time: 0.19s, train loss: 17123.7668]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 199 training [time: 0.19s, train loss: 17144.4962]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 200 training [time: 0.19s, train loss: 17165.1944]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 201 training [time: 0.19s, train loss: 17177.8403]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 202 training [time: 0.19s, train loss: 17160.1908]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 203 training [time: 0.19s, train loss: 17185.3002]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 204 training [time: 0.19s, train loss: 17180.1692]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 205 training [time: 0.19s, train loss: 17180.8245]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 206 training [time: 0.19s, train loss: 17139.9973]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 207 training [time: 0.19s, train loss: 17130.3073]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 208 training [time: 0.19s, train loss: 17117.9626]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 209 training [time: 0.19s, train loss: 17151.7112]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 210 training [time: 0.19s, train loss: 17191.8326]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 211 training [time: 0.19s, train loss: 17119.9972]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 212 training [time: 0.19s, train loss: 17134.3900]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 213 training [time: 0.19s, train loss: 17185.7908]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 214 training [time: 0.19s, train loss: 17088.2448]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 215 training [time: 0.19s, train loss: 17160.9640]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 216 training [time: 0.19s, train loss: 17129.4767]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 217 training [time: 0.19s, train loss: 17179.9672]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 218 training [time: 0.19s, train loss: 17122.3315]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 219 training [time: 0.19s, train loss: 17169.9297]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 220 training [time: 0.19s, train loss: 17098.5045]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 221 training [time: 0.19s, train loss: 17161.1445]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 222 training [time: 0.19s, train loss: 17101.2526]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 223 training [time: 0.19s, train loss: 17107.7683]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 224 training [time: 0.19s, train loss: 17145.2841]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 225 training [time: 0.19s, train loss: 17125.5245]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 226 training [time: 0.19s, train loss: 17132.1744]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 227 training [time: 0.19s, train loss: 17163.0360]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 228 training [time: 0.19s, train loss: 17112.0151]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 229 training [time: 0.19s, train loss: 17077.7994]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 230 training [time: 0.19s, train loss: 17187.7111]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 231 training [time: 0.19s, train loss: 17144.2604]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 232 training [time: 0.19s, train loss: 17171.8487]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 233 training [time: 0.19s, train loss: 17142.3425]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 234 training [time: 0.19s, train loss: 17091.7947]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 235 training [time: 0.19s, train loss: 17147.3367]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 236 training [time: 0.19s, train loss: 17154.9940]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 237 training [time: 0.19s, train loss: 17174.9567]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 238 training [time: 0.19s, train loss: 17129.9750]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 239 training [time: 0.19s, train loss: 17130.3110]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 240 training [time: 0.19s, train loss: 17159.4074]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 241 training [time: 0.19s, train loss: 17126.8076]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 242 training [time: 0.19s, train loss: 17116.0760]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 243 training [time: 0.19s, train loss: 17088.2626]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 244 training [time: 0.19s, train loss: 17180.4028]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 245 training [time: 0.19s, train loss: 17166.9718]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 246 training [time: 0.19s, train loss: 17112.0057]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 247 training [time: 0.19s, train loss: 17114.1301]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 248 training [time: 0.19s, train loss: 17116.1528]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 249 training [time: 0.19s, train loss: 17146.0063]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 250 training [time: 0.19s, train loss: 17148.6051]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 251 training [time: 0.19s, train loss: 17159.9407]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 252 training [time: 0.19s, train loss: 17147.6146]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 253 training [time: 0.19s, train loss: 17186.2891]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 254 training [time: 0.19s, train loss: 17150.4758]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 255 training [time: 0.19s, train loss: 17129.9666]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 256 training [time: 0.19s, train loss: 17134.7263]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 257 training [time: 0.19s, train loss: 17159.8481]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 258 training [time: 0.19s, train loss: 17144.2192]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 259 training [time: 0.19s, train loss: 17093.6743]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:58    INFO  epoch 260 training [time: 0.19s, train loss: 17096.2834]\n",
      "26 Dec 11:58    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 261 training [time: 0.19s, train loss: 17187.4847]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 262 training [time: 0.19s, train loss: 17101.7484]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 263 training [time: 0.19s, train loss: 17115.9641]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 264 training [time: 0.19s, train loss: 17116.4697]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 265 training [time: 0.19s, train loss: 17050.5488]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 266 training [time: 0.19s, train loss: 17163.8378]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 267 training [time: 0.19s, train loss: 17125.7548]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 268 training [time: 0.19s, train loss: 17162.3051]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 269 training [time: 0.19s, train loss: 17169.0842]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 270 training [time: 0.19s, train loss: 17131.5379]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 271 training [time: 0.19s, train loss: 17161.8838]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 272 training [time: 0.19s, train loss: 17123.2998]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 273 training [time: 0.19s, train loss: 17143.1364]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 274 training [time: 0.19s, train loss: 17129.0586]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 275 training [time: 0.19s, train loss: 17125.4250]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 276 training [time: 0.19s, train loss: 17121.8353]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 277 training [time: 0.19s, train loss: 17107.2887]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 278 training [time: 0.19s, train loss: 17117.6940]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 279 training [time: 0.19s, train loss: 17133.7069]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 280 training [time: 0.19s, train loss: 17104.7133]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 281 training [time: 0.19s, train loss: 17110.6655]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 282 training [time: 0.19s, train loss: 17184.9918]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 283 training [time: 0.19s, train loss: 17115.3907]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 284 training [time: 0.19s, train loss: 17128.9725]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 285 training [time: 0.19s, train loss: 17131.1689]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 286 training [time: 0.19s, train loss: 17113.2896]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 287 training [time: 0.19s, train loss: 17127.2432]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 288 training [time: 0.19s, train loss: 17154.3064]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 289 training [time: 0.19s, train loss: 17092.1533]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 290 training [time: 0.19s, train loss: 17141.9601]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 291 training [time: 0.19s, train loss: 17142.1132]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 292 training [time: 0.19s, train loss: 17149.9917]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 293 training [time: 0.19s, train loss: 17149.0013]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 294 training [time: 0.19s, train loss: 17060.5002]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 295 training [time: 0.19s, train loss: 17089.6376]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 296 training [time: 0.19s, train loss: 17109.5785]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 297 training [time: 0.19s, train loss: 17125.3696]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 298 training [time: 0.19s, train loss: 17102.8955]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 299 training [time: 0.19s, train loss: 17148.2440]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 300 training [time: 0.19s, train loss: 17076.8074]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 301 training [time: 0.19s, train loss: 17115.2039]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 302 training [time: 0.19s, train loss: 17122.1875]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 303 training [time: 0.19s, train loss: 17128.7885]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 304 training [time: 0.19s, train loss: 17088.4920]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 305 training [time: 0.19s, train loss: 17122.2239]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 306 training [time: 0.19s, train loss: 17120.8235]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 307 training [time: 0.19s, train loss: 17132.2637]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 308 training [time: 0.19s, train loss: 17120.0956]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 309 training [time: 0.19s, train loss: 17066.5089]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 310 training [time: 0.19s, train loss: 17034.5792]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 311 training [time: 0.19s, train loss: 17104.2286]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 312 training [time: 0.19s, train loss: 17137.5376]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 313 training [time: 0.19s, train loss: 17102.5189]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 314 training [time: 0.19s, train loss: 17097.5353]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 315 training [time: 0.19s, train loss: 17159.4165]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 316 training [time: 0.19s, train loss: 17119.3999]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 317 training [time: 0.19s, train loss: 17108.0048]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 318 training [time: 0.19s, train loss: 17064.1080]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 319 training [time: 0.19s, train loss: 17097.7358]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 320 training [time: 0.19s, train loss: 17075.9319]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 321 training [time: 0.19s, train loss: 17158.5065]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 322 training [time: 0.19s, train loss: 17123.1492]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 323 training [time: 0.19s, train loss: 17169.2234]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 324 training [time: 0.19s, train loss: 17098.9196]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 325 training [time: 0.19s, train loss: 17110.6123]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 326 training [time: 0.19s, train loss: 17142.2732]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 327 training [time: 0.19s, train loss: 17155.8938]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 328 training [time: 0.19s, train loss: 17159.6998]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 329 training [time: 0.19s, train loss: 17114.5667]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 330 training [time: 0.19s, train loss: 17101.8484]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 331 training [time: 0.19s, train loss: 17122.5646]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 332 training [time: 0.20s, train loss: 17126.2928]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 333 training [time: 0.19s, train loss: 17079.5853]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 334 training [time: 0.19s, train loss: 17080.0864]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 335 training [time: 0.19s, train loss: 17064.6927]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 336 training [time: 0.19s, train loss: 17094.3182]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 337 training [time: 0.19s, train loss: 17183.8195]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 338 training [time: 0.19s, train loss: 17112.1520]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 339 training [time: 0.19s, train loss: 17116.0012]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 340 training [time: 0.19s, train loss: 17115.8602]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 341 training [time: 0.19s, train loss: 17105.0983]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 342 training [time: 0.19s, train loss: 17146.6256]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 343 training [time: 0.19s, train loss: 17147.1372]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 344 training [time: 0.19s, train loss: 17113.5056]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 345 training [time: 0.19s, train loss: 17133.1637]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 346 training [time: 0.19s, train loss: 17107.4421]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 347 training [time: 0.19s, train loss: 17141.5908]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 348 training [time: 0.19s, train loss: 17116.9894]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 349 training [time: 0.19s, train loss: 17131.5087]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 350 training [time: 0.19s, train loss: 17087.4576]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 351 training [time: 0.19s, train loss: 17176.7878]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 352 training [time: 0.19s, train loss: 17126.5958]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 353 training [time: 0.19s, train loss: 17130.8741]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 354 training [time: 0.19s, train loss: 17111.4738]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 355 training [time: 0.19s, train loss: 17128.8689]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 356 training [time: 0.19s, train loss: 17108.1884]\n",
      "26 Dec 11:59    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 11:59    INFO  epoch 357 training [time: 0.19s, train loss: 17094.4768]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 358 training [time: 0.19s, train loss: 17109.0511]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 359 training [time: 0.19s, train loss: 17137.4275]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 360 training [time: 0.19s, train loss: 17093.8291]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 361 training [time: 0.19s, train loss: 17127.0022]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 362 training [time: 0.19s, train loss: 17128.0192]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 363 training [time: 0.19s, train loss: 17161.8527]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 364 training [time: 0.19s, train loss: 17072.9060]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 365 training [time: 0.19s, train loss: 17091.7841]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 366 training [time: 0.19s, train loss: 17148.1614]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 367 training [time: 0.21s, train loss: 17138.6475]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 368 training [time: 0.19s, train loss: 17075.4933]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 369 training [time: 0.19s, train loss: 17089.7987]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 370 training [time: 0.19s, train loss: 17052.6661]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 371 training [time: 0.19s, train loss: 17140.1183]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 372 training [time: 0.19s, train loss: 17154.8214]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 373 training [time: 0.19s, train loss: 17104.7775]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 374 training [time: 0.19s, train loss: 17146.0916]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 375 training [time: 0.19s, train loss: 17086.1611]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 376 training [time: 0.19s, train loss: 17099.8115]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 377 training [time: 0.19s, train loss: 17069.6099]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 378 training [time: 0.19s, train loss: 17151.3303]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 379 training [time: 0.19s, train loss: 17086.8660]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 380 training [time: 0.19s, train loss: 17062.1694]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 381 training [time: 0.19s, train loss: 17046.8239]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 382 training [time: 0.19s, train loss: 17091.3472]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 383 training [time: 0.19s, train loss: 17071.5125]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 384 training [time: 0.19s, train loss: 17084.1486]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 385 training [time: 0.19s, train loss: 17046.1858]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 386 training [time: 0.19s, train loss: 17121.6401]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 387 training [time: 0.19s, train loss: 17078.1168]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 388 training [time: 0.19s, train loss: 17081.4324]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 389 training [time: 0.19s, train loss: 17109.9118]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 390 training [time: 0.19s, train loss: 17105.2458]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 391 training [time: 0.19s, train loss: 17070.9764]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 392 training [time: 0.19s, train loss: 17093.9572]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 393 training [time: 0.19s, train loss: 17099.8569]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 394 training [time: 0.19s, train loss: 17108.8734]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 395 training [time: 0.19s, train loss: 17140.5879]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 396 training [time: 0.19s, train loss: 17088.7854]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 397 training [time: 0.19s, train loss: 17067.1016]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 398 training [time: 0.19s, train loss: 17141.6807]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 399 training [time: 0.20s, train loss: 17087.4023]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 400 training [time: 0.19s, train loss: 17128.6964]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 401 training [time: 0.19s, train loss: 17105.9189]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 402 training [time: 0.19s, train loss: 17122.5266]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 403 training [time: 0.19s, train loss: 17115.6418]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 404 training [time: 0.19s, train loss: 17087.1434]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 405 training [time: 0.19s, train loss: 17080.6205]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 406 training [time: 0.19s, train loss: 17098.1105]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 407 training [time: 0.19s, train loss: 17079.3818]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 408 training [time: 0.19s, train loss: 17116.7566]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 409 training [time: 0.19s, train loss: 17094.0355]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 410 training [time: 0.19s, train loss: 17108.5188]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 411 training [time: 0.19s, train loss: 17043.6360]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 412 training [time: 0.19s, train loss: 17081.4644]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 413 training [time: 0.19s, train loss: 17114.0884]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 414 training [time: 0.19s, train loss: 17066.3977]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 415 training [time: 0.19s, train loss: 17075.9108]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 416 training [time: 0.19s, train loss: 17101.7399]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 417 training [time: 0.19s, train loss: 17139.3652]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 418 training [time: 0.19s, train loss: 17116.6973]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 419 training [time: 0.19s, train loss: 17074.0475]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 420 training [time: 0.19s, train loss: 17100.9259]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 421 training [time: 0.19s, train loss: 17088.7495]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 422 training [time: 0.19s, train loss: 17095.3087]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 423 training [time: 0.19s, train loss: 17071.0535]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 424 training [time: 0.19s, train loss: 17056.4073]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 425 training [time: 0.19s, train loss: 17010.9726]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 426 training [time: 0.19s, train loss: 17112.6290]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 427 training [time: 0.19s, train loss: 17116.5522]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 428 training [time: 0.19s, train loss: 17063.3387]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 429 training [time: 0.19s, train loss: 17088.2700]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 430 training [time: 0.19s, train loss: 17093.2842]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 431 training [time: 0.19s, train loss: 17092.6829]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 432 training [time: 0.19s, train loss: 17059.6921]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 433 training [time: 0.19s, train loss: 17078.0413]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 434 training [time: 0.19s, train loss: 17088.4367]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 435 training [time: 0.19s, train loss: 17071.7664]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 436 training [time: 0.19s, train loss: 17099.7029]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 437 training [time: 0.19s, train loss: 17065.6029]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 438 training [time: 0.19s, train loss: 17101.3840]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 439 training [time: 0.19s, train loss: 17080.9320]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 440 training [time: 0.19s, train loss: 17108.6393]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 441 training [time: 0.19s, train loss: 17133.9921]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 442 training [time: 0.19s, train loss: 17056.7103]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 443 training [time: 0.19s, train loss: 17091.3735]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 444 training [time: 0.19s, train loss: 17077.7197]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 445 training [time: 0.19s, train loss: 17082.4167]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 446 training [time: 0.19s, train loss: 17050.7783]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 447 training [time: 0.19s, train loss: 17088.9697]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 448 training [time: 0.19s, train loss: 17082.7279]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 449 training [time: 0.19s, train loss: 17108.6279]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 450 training [time: 0.19s, train loss: 17064.4684]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 451 training [time: 0.19s, train loss: 17094.0967]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 452 training [time: 0.19s, train loss: 17077.2987]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 453 training [time: 0.19s, train loss: 17129.2219]\n",
      "26 Dec 12:00    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:00    INFO  epoch 454 training [time: 0.19s, train loss: 17123.0053]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 455 training [time: 0.19s, train loss: 17082.5370]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 456 training [time: 0.19s, train loss: 17063.5830]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 457 training [time: 0.19s, train loss: 17087.5492]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 458 training [time: 0.19s, train loss: 17095.4749]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 459 training [time: 0.19s, train loss: 17117.2770]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 460 training [time: 0.19s, train loss: 17049.0526]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 461 training [time: 0.19s, train loss: 17094.3420]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 462 training [time: 0.19s, train loss: 17100.6826]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 463 training [time: 0.19s, train loss: 17089.8059]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 464 training [time: 0.19s, train loss: 17069.4958]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 465 training [time: 0.19s, train loss: 17101.4176]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 466 training [time: 0.19s, train loss: 17106.5155]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 467 training [time: 0.19s, train loss: 17045.7457]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 468 training [time: 0.19s, train loss: 17044.9271]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 469 training [time: 0.19s, train loss: 17093.4270]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 470 training [time: 0.19s, train loss: 17085.3207]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 471 training [time: 0.19s, train loss: 17085.3473]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 472 training [time: 0.19s, train loss: 17103.9893]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 473 training [time: 0.19s, train loss: 17049.8232]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 474 training [time: 0.19s, train loss: 17141.9485]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 475 training [time: 0.19s, train loss: 17115.3741]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 476 training [time: 0.19s, train loss: 17078.1910]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 477 training [time: 0.19s, train loss: 17107.3015]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 478 training [time: 0.19s, train loss: 17076.9014]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 479 training [time: 0.19s, train loss: 17042.1788]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 480 training [time: 0.19s, train loss: 17123.8774]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 481 training [time: 0.19s, train loss: 17115.1472]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 482 training [time: 0.19s, train loss: 17090.2069]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 483 training [time: 0.19s, train loss: 17085.0151]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 484 training [time: 0.19s, train loss: 17072.8833]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 485 training [time: 0.19s, train loss: 17058.0167]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 486 training [time: 0.19s, train loss: 17093.9709]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 487 training [time: 0.19s, train loss: 17116.3573]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 488 training [time: 0.19s, train loss: 17040.0092]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 489 training [time: 0.19s, train loss: 17123.4703]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 490 training [time: 0.19s, train loss: 17098.3389]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 491 training [time: 0.19s, train loss: 17051.5371]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 492 training [time: 0.19s, train loss: 17085.0159]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 493 training [time: 0.19s, train loss: 17096.4445]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 494 training [time: 0.19s, train loss: 17077.6445]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 495 training [time: 0.19s, train loss: 17093.5327]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 496 training [time: 0.19s, train loss: 17060.5715]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 497 training [time: 0.19s, train loss: 17071.9667]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 498 training [time: 0.19s, train loss: 17095.4172]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n",
      "26 Dec 12:01    INFO  epoch 499 training [time: 0.19s, train loss: 17055.0522]\n",
      "26 Dec 12:01    INFO  Saving current: saved/MultiVAE-Dec-26-2022_11-56-18.pth\n"
     ]
    }
   ],
   "source": [
    "# trainer loading and initialization\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data, valid_data, saved=True, show_progress=config['show_progress']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cec8a633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:55:09.385714Z",
     "start_time": "2022-12-23T06:55:09.382683Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"last\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79572b1",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54c9f3b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:56:10.435390Z",
     "start_time": "2022-12-23T06:56:10.432244Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path='./saved/MultiVAE-Dec-26-2022_11-56-18.pth'\n",
    "# rank K 설정\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9753e240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T02:01:25.573389Z",
     "start_time": "2022-12-23T01:59:54.577168Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26 Dec 12:03    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "26 Dec 12:03    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [1, 0, 0]}, 'group_by': 'user', 'order': 'RO', 'mode': 'full'}]\n",
      "Inference:   0%|          | 0/245 [00:00<?, ?it/s]:   2%|▏         | 6/245 [00:00<00:04, 59.50it/s]:   5%|▌         | 13/245 [00:00<00:03, 60.85it/s]:   8%|▊         | 20/245 [00:00<00:03, 61.11it/s]:  11%|█         | 27/245 [00:00<00:03, 60.78it/s]:  14%|█▍        | 34/245 [00:00<00:03, 60.42it/s]:  17%|█▋        | 41/245 [00:00<00:03, 60.57it/s]:  20%|█▉        | 48/245 [00:00<00:03, 60.80it/s]:  22%|██▏       | 55/245 [00:00<00:03, 60.85it/s]:  25%|██▌       | 62/245 [00:01<00:03, 60.89it/s]:  28%|██▊       | 69/245 [00:01<00:02, 60.78it/s]:  31%|███       | 76/245 [00:01<00:02, 59.84it/s]:  34%|███▍      | 83/245 [00:01<00:02, 60.66it/s]:  37%|███▋      | 90/245 [00:01<00:02, 59.52it/s]:  39%|███▉      | 96/245 [00:01<00:02, 59.40it/s]:  42%|████▏     | 103/245 [00:01<00:02, 59.90it/s]:  44%|████▍     | 109/245 [00:01<00:02, 58.81it/s]:  47%|████▋     | 116/245 [00:01<00:02, 59.43it/s]:  50%|████▉     | 122/245 [00:02<00:02, 58.54it/s]:  53%|█████▎    | 129/245 [00:02<00:01, 59.45it/s]:  55%|█████▌    | 135/245 [00:02<00:01, 59.06it/s]:  58%|█████▊    | 141/245 [00:02<00:01, 59.11it/s]:  60%|██████    | 147/245 [00:02<00:01, 59.32it/s]:  62%|██████▏   | 153/245 [00:02<00:01, 59.46it/s]:  65%|██████▍   | 159/245 [00:02<00:01, 59.53it/s]:  67%|██████▋   | 165/245 [00:02<00:01, 59.25it/s]:  70%|███████   | 172/245 [00:02<00:01, 59.68it/s]:  73%|███████▎  | 178/245 [00:02<00:01, 59.53it/s]:  75%|███████▌  | 184/245 [00:03<00:01, 59.30it/s]:  78%|███████▊  | 190/245 [00:03<00:00, 59.42it/s]:  80%|████████  | 196/245 [00:03<00:00, 59.49it/s]:  82%|████████▏ | 202/245 [00:03<00:00, 59.52it/s]:  85%|████████▍ | 208/245 [00:03<00:00, 58.73it/s]:  87%|████████▋ | 214/245 [00:03<00:00, 58.56it/s]:  90%|████████▉ | 220/245 [00:03<00:00, 58.86it/s]:  92%|█████████▏| 226/245 [00:03<00:00, 58.86it/s]:  95%|█████████▍| 232/245 [00:03<00:00, 58.80it/s]:  97%|█████████▋| 238/245 [00:03<00:00, 58.97it/s]: 100%|█████████▉| 244/245 [00:04<00:00, 58.99it/s]: 100%|██████████| 245/245 [00:04<00:00, 59.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference done!\n"
     ]
    }
   ],
   "source": [
    "# config, model, dataset 불러오기\n",
    "checkpoint = torch.load(model_path)\n",
    "config = checkpoint['config']\n",
    "config['dataset'] = 'train_data'\n",
    "\n",
    "dataset = create_dataset(config)\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "model = get_model(config['model'])(config, test_data.dataset).to(config['device'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.load_other_parameter(checkpoint.get('other_parameter'))\n",
    "\n",
    "# device 설정\n",
    "device = config.final_config_dict['device']\n",
    "\n",
    "# user, item id -> token 변환 array\n",
    "user_id = config['USER_ID_FIELD']\n",
    "item_id = config['ITEM_ID_FIELD']\n",
    "user_id2token = dataset.field2id_token[user_id]\n",
    "item_id2token = dataset.field2id_token[item_id]\n",
    "\n",
    "# user id list\n",
    "all_user_list = torch.arange(1, len(user_id2token)).view(-1,128) # 245, 128\n",
    "\n",
    "# user, item 길이\n",
    "user_len = len(user_id2token) # 31361 (PAD 포함)\n",
    "item_len = len(item_id2token) # 6808 (PAD 포함)\n",
    "\n",
    "# user-item sparse matrix\n",
    "matrix = dataset.inter_matrix(form='csr') # (31361, 6808)\n",
    "\n",
    "# user id, predict item id 저장 변수\n",
    "pred_list = None\n",
    "user_list = None\n",
    "\n",
    "# model 평가모드 전환\n",
    "model.eval()\n",
    "\n",
    "# progress bar 설정\n",
    "tbar = tqdm(all_user_list, desc=set_color(f\"Inference\", 'pink')) # 245, 128\n",
    "\n",
    "for data in tbar: # data: 128, \n",
    "    # interaction 생성\n",
    "    interaction = dict()\n",
    "    interaction = Interaction(interaction)\n",
    "    interaction[user_id] = data\n",
    "    interaction = interaction.to(device)\n",
    "\n",
    "    # user item별 score 예측\n",
    "    score = model.full_sort_predict(interaction) # [1, 871424]\n",
    "    score = score.view(-1, item_len) # 128, 6808\n",
    "\n",
    "    rating_pred = score.cpu().data.numpy().copy() # 128, 6808\n",
    "\n",
    "    user_index = data.numpy() # 128,\n",
    "\n",
    "    # idx에는 128명의 영화상호작용이 True, False로 있다.\n",
    "    idx = matrix[user_index].toarray() > 0 # idx shape: 128, 6808\n",
    "\n",
    "    rating_pred[idx] = -np.inf # idx에서 True부분이 -inf로 변경\n",
    "    rating_pred[:, 0] = -np.inf # 첫번째 PAD 열도 -inf로 변경\n",
    "    \n",
    "    # np.argpartition(배열, -K) : 배열에서 순서 상관없이 큰 값 K개를 뽑아 오른쪽에 놓겠다 -> 인덱스반환\n",
    "    # rating_pred에서 각 행마다 K개의 score가 큰 인덱스를 오른쪽에 두고, 그 K개만 가져오기\n",
    "    ind = np.argpartition(rating_pred, -K)[:, -K:] # rating_pred: (128, 6808) -> ind: (128, 20)\n",
    "\n",
    "    user_row_index = np.arange(len(rating_pred)).reshape(-1,1) # [[0],[1],...,[127]]\n",
    "    arr_ind = rating_pred[user_row_index, ind] # 128, 6808 -> 128, 20\n",
    "\n",
    "    # arr_ind 내부에서 행별로, 내림차순 정렬해서 index 나오도록\n",
    "    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n",
    "\n",
    "    # ind는 item의 real index를 갖는 128,20 -> arr_ind_argsort를 통해 pred가 높은 상위 20개 read index 추출\n",
    "    batch_pred_list = ind[user_row_index, arr_ind_argsort] # 128,20 -> 128,20\n",
    "\n",
    "    if pred_list is None: # 처음에는 직접 정의\n",
    "        pred_list = batch_pred_list\n",
    "        user_list = user_index\n",
    "    else: # pred_list가 있을 때는, append\n",
    "        pred_list = np.append(pred_list, batch_pred_list, axis=0)\n",
    "        user_list = np.append(\n",
    "            user_list, user_index, axis=0\n",
    "        )\n",
    "\n",
    "result = []\n",
    "for user, pred in zip(user_list, pred_list):\n",
    "    for item in pred:\n",
    "        result.append((int(user_id2token[user]), int(item_id2token[item])))\n",
    "\n",
    "# 데이터 저장\n",
    "sub = pd.DataFrame(result, columns=[\"user\", \"item\"])\n",
    "sub.to_csv(\n",
    "    \"submission.csv\", index=False\n",
    ")\n",
    "print('inference done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a19ad",
   "metadata": {},
   "source": [
    "## 제출파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "694acad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c221cde8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:04:20.200880Z",
     "start_time": "2022-12-21T15:04:19.735937Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.user = sub.user.map(uidx2user)\n",
    "sub.item = sub.item.map(iidx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e53b487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:04:20.200880Z",
     "start_time": "2022-12-21T15:04:19.735937Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('MultiVAE_1_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab3190bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:04:20.213053Z",
     "start_time": "2022-12-21T15:04:20.203992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>79132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>68954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>7502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313595</th>\n",
       "      <td>138493</td>\n",
       "      <td>4226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313596</th>\n",
       "      <td>138493</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313597</th>\n",
       "      <td>138493</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313598</th>\n",
       "      <td>138493</td>\n",
       "      <td>33794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313599</th>\n",
       "      <td>138493</td>\n",
       "      <td>7502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user   item\n",
       "0           11  79132\n",
       "1           11     50\n",
       "2           11    858\n",
       "3           11  68954\n",
       "4           11   7502\n",
       "...        ...    ...\n",
       "313595  138493   4226\n",
       "313596  138493    858\n",
       "313597  138493    593\n",
       "313598  138493  33794\n",
       "313599  138493   7502\n",
       "\n",
       "[313600 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70286f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:05:21.943304Z",
     "start_time": "2022-12-21T15:05:21.906462Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.user = sub.user.map(user2idx)\n",
    "sub.item = sub.item.map(item2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bf1ebf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:18:58.572093Z",
     "start_time": "2022-12-21T15:18:58.563656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2505</td>\n",
       "      <td>1230782529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>1230782534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>1230782539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item        time\n",
       "0     0  2505  1230782529\n",
       "1     0   109  1230782534\n",
       "2     0   319  1230782539"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns=['user','item','time']\n",
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53b24294",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['user','item','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb8398ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:05:25.644540Z",
     "start_time": "2022-12-21T15:05:23.939273Z"
    }
   },
   "outputs": [],
   "source": [
    "afterdf = sub.merge(train[['user','item','time']], on=['user','item'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa056491",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/MR/Recbole/Recbole_juj.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6f766965526563227d/opt/ml/input/MR/Recbole/Recbole_juj.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m afterdf[\u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x : x\u001b[39m.\u001b[39;49mvalue_counts())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1138\u001b[0m             values,\n\u001b[1;32m   1139\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1141\u001b[0m         )\n\u001b[1;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/opt/ml/input/MR/Recbole/Recbole_juj.ipynb Cell 40\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6f766965526563227d/opt/ml/input/MR/Recbole/Recbole_juj.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m afterdf[\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x : x\u001b[39m.\u001b[39;49mvalue_counts())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "afterdf['user'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdad93e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user       0.0\n",
       "item    6185.0\n",
       "time       NaN\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = \n",
    "afterdf.loc[,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f3972",
   "metadata": {},
   "source": [
    "## full_sort_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f028083e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:27:56.495456Z",
     "start_time": "2022-12-23T05:27:56.157658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.3M\r\n",
      "drwxr-xr-x 2 root root 4.0K Dec 22 21:53 .\r\n",
      "drwxr-xr-x 5 root root 4.0K Dec 23 00:19 ..\r\n",
      "-rw-r--r-- 1 root root 1.3M Dec 22 22:01 sequential_data.inter\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh dataset/sequential_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b6236429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:29:39.635079Z",
     "start_time": "2022-12-23T05:29:39.632016Z"
    }
   },
   "outputs": [],
   "source": [
    "config['eval_args'] = {'split': {'RS': [1, 0, 99]},\n",
    "                         'group_by': 'user',\n",
    "                         'order': 'RO',\n",
    "                         'mode': 'full'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0501192a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:30:25.284207Z",
     "start_time": "2022-12-23T05:29:40.007161Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = create_dataset(config)\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3cf1c40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:30:27.114403Z",
     "start_time": "2022-12-23T05:30:25.286511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtrain_data\u001b[0m\n",
       "\u001b[1;34mThe number of users\u001b[0m: 31361\n",
       "\u001b[1;34mAverage actions of users\u001b[0m: 162.26514668367346\n",
       "\u001b[1;34mThe number of items\u001b[0m: 6808\n",
       "\u001b[1;34mAverage actions of items\u001b[0m: 747.5591303070369\n",
       "\u001b[1;34mThe number of inters\u001b[0m: 5088635\n",
       "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 97.61662789986185%\n",
       "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp', 'label']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ffa3909f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:33:04.298711Z",
     "start_time": "2022-12-23T05:32:31.410615Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_list2 = None\n",
    "user_list2 = []\n",
    "from recbole.utils.case_study import full_sort_topk\n",
    "for data in tbar:\n",
    "    batch_pred_list2 = full_sort_topk(data, model, test_data, 10, device=device)[1]\n",
    "    batch_pred_list2 = batch_pred_list2.clone().detach().cpu().numpy()\n",
    "    if pred_list2 is None:\n",
    "        pred_list2 = batch_pred_list2\n",
    "        user_list2 = data.numpy()\n",
    "    else:\n",
    "        pred_list2 = np.append(pred_list2, batch_pred_list2, axis=0)\n",
    "        user_list2 = np.append(\n",
    "            user_list2, data.numpy(), axis=0\n",
    "        )\n",
    "tbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fecbb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e52dc8c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:37:25.199548Z",
     "start_time": "2022-12-23T05:37:24.455931Z"
    }
   },
   "outputs": [],
   "source": [
    "# user별 item 추천 결과 하나로 합쳐주기\n",
    "result2 = []\n",
    "for user, pred in zip(user_list2, pred_list2):\n",
    "    for item in pred:\n",
    "        result2.append((int(user_id2token[user]), int(item_id2token[item])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a38598ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:43:26.662260Z",
     "start_time": "2022-12-23T05:43:26.479789Z"
    }
   },
   "outputs": [],
   "source": [
    "sub2 = pd.DataFrame(result2, columns=[\"user\", \"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3526794d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:43:27.455395Z",
     "start_time": "2022-12-23T05:43:27.419044Z"
    }
   },
   "outputs": [],
   "source": [
    "sub2.user = sub2.user.map(uidx2user)\n",
    "sub2.item = sub2.item.map(iidx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "05b549c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:43:29.037970Z",
     "start_time": "2022-12-23T05:43:29.029143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>34048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313595</th>\n",
       "      <td>138493</td>\n",
       "      <td>4306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313596</th>\n",
       "      <td>138493</td>\n",
       "      <td>5218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313597</th>\n",
       "      <td>138493</td>\n",
       "      <td>7361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313598</th>\n",
       "      <td>138493</td>\n",
       "      <td>4973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313599</th>\n",
       "      <td>138493</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user   item\n",
       "0           11    780\n",
       "1           11    480\n",
       "2           11   1270\n",
       "3           11   1214\n",
       "4           11  34048\n",
       "...        ...    ...\n",
       "313595  138493   4306\n",
       "313596  138493   5218\n",
       "313597  138493   7361\n",
       "313598  138493   4973\n",
       "313599  138493   1210\n",
       "\n",
       "[313600 rows x 2 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6fb5c944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:43:33.743066Z",
     "start_time": "2022-12-23T05:43:33.730181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub2.item[0] in train[train.user==11].item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "50f55d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T05:43:49.750233Z",
     "start_time": "2022-12-23T05:43:49.678648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>34048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>8644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>6502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>5349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user   item\n",
       "0    11    780\n",
       "1    11    480\n",
       "2    11   1270\n",
       "3    11   1214\n",
       "4    11  34048\n",
       "5    11   8644\n",
       "6    11      1\n",
       "7    11    367\n",
       "8    11   6502\n",
       "9    11   5349"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for uid in sorted(set(sub2.user)):\n",
    "    sub2[(sub2.user==uid) & sub2.item.isin(train[train.user==uid].item)]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76506ae7",
   "metadata": {},
   "source": [
    "## 시간 순서 잘 지켜졌는지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ef26d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:20:27.027886Z",
     "start_time": "2022-12-21T15:20:26.716689Z"
    }
   },
   "outputs": [],
   "source": [
    "train.user = train.user.map(uidx2user)\n",
    "train.item = train.item.map(iidx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fab932ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:39:03.101594Z",
     "start_time": "2022-12-21T15:39:03.098416Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from time import localtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5a4a7f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:59:09.403221Z",
     "start_time": "2022-12-21T15:59:09.361733Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../../data/train'\n",
    "year_data = pd.read_csv(os.path.join(data_path, 'years.tsv'), sep='\\t')\n",
    "writer_data = pd.read_csv(os.path.join(data_path, 'writers.tsv'), sep='\\t')\n",
    "title_data = pd.read_csv(os.path.join(data_path, 'titles.tsv'), sep='\\t')\n",
    "genre_data = pd.read_csv(os.path.join(data_path, 'genres.tsv'), sep='\\t')\n",
    "director_data = pd.read_csv(os.path.join(data_path, 'directors.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263311f",
   "metadata": {},
   "source": [
    "### train에 review_year, month, day, year_month 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1e85d4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:40:31.275714Z",
     "start_time": "2022-12-21T15:40:25.781666Z"
    }
   },
   "outputs": [],
   "source": [
    "train['review_year'] = train['time'].apply(lambda x : localtime(x).tm_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7f2e489c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:40:51.473586Z",
     "start_time": "2022-12-21T15:40:45.776149Z"
    }
   },
   "outputs": [],
   "source": [
    "train['month'] = train['time'].apply(lambda x : localtime(x).tm_mon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b6d31858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T15:40:57.016193Z",
     "start_time": "2022-12-21T15:40:51.476743Z"
    }
   },
   "outputs": [],
   "source": [
    "train['day'] = train['time'].apply(lambda x : localtime(x).tm_mday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "26e44e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:04:34.188614Z",
     "start_time": "2022-12-21T16:04:26.152369Z"
    }
   },
   "outputs": [],
   "source": [
    "train['year_month'] = train['time'].apply(lambda x : time.strftime('%Y-%m',localtime(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e97ef3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:04:52.690134Z",
     "start_time": "2022-12-21T16:04:47.387754Z"
    }
   },
   "outputs": [],
   "source": [
    "train.sort_values(['user','time'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fde34908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:05:43.898586Z",
     "start_time": "2022-12-21T16:05:43.588994Z"
    }
   },
   "outputs": [],
   "source": [
    "user2lastyear = dict(train.groupby('user').year.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7d16d273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:05:48.642649Z",
     "start_time": "2022-12-21T16:05:44.450809Z"
    }
   },
   "outputs": [],
   "source": [
    "user2lastyearmonth = dict(train.groupby('user').year_month.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e4bd5b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:06:07.704108Z",
     "start_time": "2022-12-21T16:06:07.545014Z"
    }
   },
   "outputs": [],
   "source": [
    "train['lastyear']=train.user.map(user2lastyear)\n",
    "train['last_yearmonth']=train.user.map(user2lastyearmonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "370e8f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:06:26.122897Z",
     "start_time": "2022-12-21T16:06:26.107413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year_month</th>\n",
       "      <th>lastyear</th>\n",
       "      <th>last_yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4643</td>\n",
       "      <td>1230782529</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>170</td>\n",
       "      <td>1230782534</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>531</td>\n",
       "      <td>1230782539</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>616</td>\n",
       "      <td>1230782542</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2140</td>\n",
       "      <td>1230782563</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154466</th>\n",
       "      <td>138493</td>\n",
       "      <td>44022</td>\n",
       "      <td>1260209449</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154467</th>\n",
       "      <td>138493</td>\n",
       "      <td>4958</td>\n",
       "      <td>1260209482</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154468</th>\n",
       "      <td>138493</td>\n",
       "      <td>68319</td>\n",
       "      <td>1260209720</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154469</th>\n",
       "      <td>138493</td>\n",
       "      <td>40819</td>\n",
       "      <td>1260209726</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154470</th>\n",
       "      <td>138493</td>\n",
       "      <td>27311</td>\n",
       "      <td>1260209807</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5154471 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user   item        time  year  month  day year_month  lastyear  \\\n",
       "0            11   4643  1230782529  2009      1    1    2009-01      2011   \n",
       "1            11    170  1230782534  2009      1    1    2009-01      2011   \n",
       "2            11    531  1230782539  2009      1    1    2009-01      2011   \n",
       "3            11    616  1230782542  2009      1    1    2009-01      2011   \n",
       "4            11   2140  1230782563  2009      1    1    2009-01      2011   \n",
       "...         ...    ...         ...   ...    ...  ...        ...       ...   \n",
       "5154466  138493  44022  1260209449  2009     12    7    2009-12      2009   \n",
       "5154467  138493   4958  1260209482  2009     12    7    2009-12      2009   \n",
       "5154468  138493  68319  1260209720  2009     12    7    2009-12      2009   \n",
       "5154469  138493  40819  1260209726  2009     12    7    2009-12      2009   \n",
       "5154470  138493  27311  1260209807  2009     12    7    2009-12      2009   \n",
       "\n",
       "        last_yearmonth  \n",
       "0              2011-01  \n",
       "1              2011-01  \n",
       "2              2011-01  \n",
       "3              2011-01  \n",
       "4              2011-01  \n",
       "...                ...  \n",
       "5154466        2009-12  \n",
       "5154467        2009-12  \n",
       "5154468        2009-12  \n",
       "5154469        2009-12  \n",
       "5154470        2009-12  \n",
       "\n",
       "[5154471 rows x 9 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6e034",
   "metadata": {},
   "source": [
    "### year 채워넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "171162e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:13:31.878514Z",
     "start_time": "2022-12-21T16:13:31.865559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "      <th>review_year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year_month</th>\n",
       "      <th>lastyear</th>\n",
       "      <th>last_yearmonth</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4643</td>\n",
       "      <td>1230782529</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>Planet of the Apes (2001)</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>170</td>\n",
       "      <td>1230782534</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>Hackers (1995)</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>531</td>\n",
       "      <td>1230782539</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>Secret Garden, The (1993)</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>616</td>\n",
       "      <td>1230782542</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>Aristocats, The (1970)</td>\n",
       "      <td>1970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2140</td>\n",
       "      <td>1230782563</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>Dark Crystal, The (1982)</td>\n",
       "      <td>1982.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item        time  review_year  month  day year_month  lastyear  \\\n",
       "0    11  4643  1230782529         2009      1    1    2009-01      2011   \n",
       "1    11   170  1230782534         2009      1    1    2009-01      2011   \n",
       "2    11   531  1230782539         2009      1    1    2009-01      2011   \n",
       "3    11   616  1230782542         2009      1    1    2009-01      2011   \n",
       "4    11  2140  1230782563         2009      1    1    2009-01      2011   \n",
       "\n",
       "  last_yearmonth                      title    year  \n",
       "0        2011-01  Planet of the Apes (2001)  2001.0  \n",
       "1        2011-01             Hackers (1995)  1995.0  \n",
       "2        2011-01  Secret Garden, The (1993)  1993.0  \n",
       "3        2011-01     Aristocats, The (1970)  1970.0  \n",
       "4        2011-01   Dark Crystal, The (1982)  1982.0  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df = train.copy()\n",
    "side_info = [title_data,year_data]\n",
    "for side in side_info:\n",
    "    merge_df = merge_df.merge(side,how = 'left',on='item')\n",
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "191d1ea7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:15:00.337846Z",
     "start_time": "2022-12-21T16:14:58.654866Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_df['year_from_title'] = merge_df['title'].apply(lambda x : (x[-5:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "382ccb15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:15:08.267235Z",
     "start_time": "2022-12-21T16:15:07.916134Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_df.loc[merge_df['year_from_title']=='007-','year_from_title'] = 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4c297be5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:15:29.614558Z",
     "start_time": "2022-12-21T16:15:29.007110Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_df.year_from_title=merge_df.year_from_title.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b825e518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:15:52.333958Z",
     "start_time": "2022-12-21T16:15:52.317870Z"
    }
   },
   "outputs": [],
   "source": [
    "cond = merge_df.year_from_title == merge_df.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9b4afdbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:19:15.657657Z",
     "start_time": "2022-12-21T16:19:15.635197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fawlty Towers (1975-1979)    163\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df[~cond & merge_df.year.notna()].title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2aa627c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:19:44.286699Z",
     "start_time": "2022-12-21T16:19:44.165906Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_df.year = merge_df.year.fillna(merge_df.year_from_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "909ff5c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:21:59.685339Z",
     "start_time": "2022-12-21T16:21:59.604779Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_df.year = merge_df.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4ae107aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:25:24.129697Z",
     "start_time": "2022-12-21T16:25:24.104178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "      <th>review_year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year_month</th>\n",
       "      <th>lastyear</th>\n",
       "      <th>last_yearmonth</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>year_from_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3064009</th>\n",
       "      <td>81663</td>\n",
       "      <td>91535</td>\n",
       "      <td>1323223207</td>\n",
       "      <td>2011</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2011-12</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-12</td>\n",
       "      <td>Bourne Legacy, The (2012)</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617327</th>\n",
       "      <td>123609</td>\n",
       "      <td>89745</td>\n",
       "      <td>1316645274</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>2011-09</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-09</td>\n",
       "      <td>Avengers, The (2012)</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user   item        time  review_year  month  day year_month  \\\n",
       "3064009   81663  91535  1323223207         2011     12    7    2011-12   \n",
       "4617327  123609  89745  1316645274         2011      9   21    2011-09   \n",
       "\n",
       "         lastyear last_yearmonth                      title  year  \\\n",
       "3064009      2011        2011-12  Bourne Legacy, The (2012)  2012   \n",
       "4617327      2011        2011-09       Avengers, The (2012)  2012   \n",
       "\n",
       "         year_from_title  \n",
       "3064009             2012  \n",
       "4617327             2012  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df[merge_df.lastyear < merge_df.year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "58ec6d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:23:03.437285Z",
     "start_time": "2022-12-21T16:23:03.039795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6807"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## item별 year이 모두 동일한지 첫번째값과 평균값이 같은 아이템들 확인 -> 모두 일치\n",
    "sum(merge_df.groupby('item').year.first() == merge_df.groupby('item').year.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9de46561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:23:26.643743Z",
     "start_time": "2022-12-21T16:23:26.425009Z"
    }
   },
   "outputs": [],
   "source": [
    "item2year = dict(merge_df.groupby('item').year.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614bfa19",
   "metadata": {},
   "source": [
    "## sub 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e9b0029e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:34:41.301630Z",
     "start_time": "2022-12-21T16:34:41.255076Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.user = sub.user.map(uidx2user)\n",
    "sub.item = sub.item.map(iidx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "458091f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:34:42.082243Z",
     "start_time": "2022-12-21T16:34:42.010828Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['lastyear']=sub.user.map(user2lastyear)\n",
    "sub['last_yearmonth']=sub.user.map(user2lastyearmonth)\n",
    "sub['m_year'] = sub.item.map(item2year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "8381ee69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:34:42.794898Z",
     "start_time": "2022-12-21T16:34:42.784216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>lastyear</th>\n",
       "      <th>last_yearmonth</th>\n",
       "      <th>m_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4370</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4886</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>32587</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>40815</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627195</th>\n",
       "      <td>138493</td>\n",
       "      <td>4720</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627196</th>\n",
       "      <td>138493</td>\n",
       "      <td>293</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627197</th>\n",
       "      <td>138493</td>\n",
       "      <td>2174</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627198</th>\n",
       "      <td>138493</td>\n",
       "      <td>4848</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627199</th>\n",
       "      <td>138493</td>\n",
       "      <td>4963</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-12</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user   item  lastyear last_yearmonth  m_year\n",
       "0           11   4370      2011        2011-01    2001\n",
       "1           11   4886      2011        2011-01    2001\n",
       "2           11     47      2011        2011-01    1995\n",
       "3           11  32587      2011        2011-01    2005\n",
       "4           11  40815      2011        2011-01    2005\n",
       "...        ...    ...       ...            ...     ...\n",
       "627195  138493   4720      2009        2009-12    2001\n",
       "627196  138493    293      2009        2009-12    1994\n",
       "627197  138493   2174      2009        2009-12    1988\n",
       "627198  138493   4848      2009        2009-12    2001\n",
       "627199  138493   4963      2009        2009-12    2001\n",
       "\n",
       "[627200 rows x 5 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "063f4921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:35:21.577364Z",
     "start_time": "2022-12-21T16:35:21.540878Z"
    }
   },
   "outputs": [],
   "source": [
    "sub2 = sub[sub.lastyear >= sub.m_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0a856c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:37:51.278536Z",
     "start_time": "2022-12-21T16:37:51.222509Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('EASE_1_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "575d5324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:38:36.735473Z",
     "start_time": "2022-12-21T16:38:36.690150Z"
    }
   },
   "outputs": [],
   "source": [
    "df['item2']= sub2.groupby('user').item.head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad547288",
   "metadata": {},
   "source": [
    "### 이전 제출(1594)대비 현재 제출(1595)에서 달라진 부분 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0423bfe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T17:08:12.245084Z",
     "start_time": "2022-12-21T17:08:10.690484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 313600/313600 [00:00<00:00, 562116.84it/s]\n"
     ]
    }
   ],
   "source": [
    "## 이전 제출(1594)대비 현재 제출(1595)에서 달라진 부분 보기\n",
    "idx_list=[]\n",
    "user_item2 = dict(df.groupby('user').item2.apply(list))\n",
    "for i in tqdm(df.itertuples(), total=df.shape[0]):\n",
    "    if i.item not in user_item2[i.user]:\n",
    "        idx_list.append(i.Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b0bef36c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T17:09:16.372543Z",
     "start_time": "2022-12-21T17:09:16.308170Z"
    }
   },
   "outputs": [],
   "source": [
    "df['lastyear']=df.user.map(user2lastyear)\n",
    "df['last_yearmonth']=df.user.map(user2lastyearmonth)\n",
    "df['m_year'] = df.item.map(item2year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "70a6f63b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T17:09:17.092727Z",
     "start_time": "2022-12-21T17:09:17.063107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>item2</th>\n",
       "      <th>lastyear</th>\n",
       "      <th>last_yearmonth</th>\n",
       "      <th>m_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>61</td>\n",
       "      <td>58559</td>\n",
       "      <td>48780</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-12</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>664</td>\n",
       "      <td>63436</td>\n",
       "      <td>8528</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-11</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>915</td>\n",
       "      <td>54286</td>\n",
       "      <td>4027</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006-08</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3504</th>\n",
       "      <td>1539</td>\n",
       "      <td>79132</td>\n",
       "      <td>3949</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-07</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5019</th>\n",
       "      <td>2160</td>\n",
       "      <td>58559</td>\n",
       "      <td>111</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-01</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300758</th>\n",
       "      <td>132448</td>\n",
       "      <td>69844</td>\n",
       "      <td>2542</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-10</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307115</th>\n",
       "      <td>135535</td>\n",
       "      <td>58559</td>\n",
       "      <td>318</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005-12</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307702</th>\n",
       "      <td>135798</td>\n",
       "      <td>88125</td>\n",
       "      <td>6539</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311225</th>\n",
       "      <td>137460</td>\n",
       "      <td>58559</td>\n",
       "      <td>745</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-09</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312617</th>\n",
       "      <td>138094</td>\n",
       "      <td>79132</td>\n",
       "      <td>4973</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-10</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user   item  item2  lastyear last_yearmonth  m_year\n",
       "109         61  58559  48780      2007        2007-12    2008\n",
       "1559       664  63436   8528      2007        2007-11    2008\n",
       "2144       915  54286   4027      2006        2006-08    2007\n",
       "3504      1539  79132   3949      2009        2009-07    2010\n",
       "5019      2160  58559    111      2007        2007-01    2008\n",
       "...        ...    ...    ...       ...            ...     ...\n",
       "300758  132448  69844   2542      2007        2007-10    2009\n",
       "307115  135535  58559    318      2005        2005-12    2008\n",
       "307702  135798  88125   6539      2010        2010-12    2011\n",
       "311225  137460  58559    745      2007        2007-09    2008\n",
       "312617  138094  79132   4973      2009        2009-10    2010\n",
       "\n",
       "[213 rows x 6 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[idx_list,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb6494",
   "metadata": {},
   "source": [
    "## 최종 제출 -> 1595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b9e0806a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T16:40:18.486363Z",
     "start_time": "2022-12-21T16:40:18.058834Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['user','item2']].rename(columns={'item2':'item'}).to_csv(\"EASE_1_0_Top20_remove_review_after_movie.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "764px",
    "left": "26px",
    "top": "111.141px",
    "width": "393.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
